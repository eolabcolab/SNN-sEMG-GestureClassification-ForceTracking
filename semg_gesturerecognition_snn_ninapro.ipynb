{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77d8TBxVP8fM"
      },
      "source": [
        "# **sEMG-based Gesture Recognition with Spiking Neural Networks**\n",
        "\n",
        "This project leverages surface Electromyography (sEMG) signals for precise hand gesture recognition using Spiking Neural Networks (SNNs).\n",
        "\n",
        "The training and validation are based on the NinaPro dataset, which encompasses sEMG and kinematic data from different subjects performing 52 distinct hand movements. The Delta modulation approach is applied to both the raw signal and its first and second derivative.\n",
        "\n",
        "For the neural network management, SLAYER libraries are employed. The chosen neuron model is the Leaky Integrate and Fire (LIF) or the CUrrent-Based Alpha (CUBA)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_LhNZtEP3sd"
      },
      "source": [
        "## LAVA framework in Colab\n",
        "LAVA (Loihi Advanced Virtualized Architecture) is a software framework developed by Intel, tailored for the development and simulation of Spiking Neural Networks (SNNs) and neuromorphic computing applications. It is intricately linked with Intel's Loihi, a neuromorphic computing chip designed to mimic biological brain functioning, offering energy-efficient processing and real-time learning capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjG3W4CEPzeg",
        "outputId": "02e70659-f423-4489-d42d-a3ccf4f1bffc"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  # Import google.colab to check if running in Colab\n",
        "  import google.colab\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive/')\n",
        "  !pip install /content/gdrive/MyDrive/Library/lava-dl-main.zip -q\n",
        "  import os\n",
        "  os.kill(os.getpid(), 9)\n",
        "\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb65IxBc3Vyq"
      },
      "source": [
        "## Dependencies and functions\n",
        "In this section of the notebook, we focus on setting up the foundational components necessary for our project. This includes importing libraries and defining essential functions that will be used throughout the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4ORPcaAC3_R"
      },
      "source": [
        "### Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nieG9YVbCjMQ",
        "outputId": "54395eab-6321-460f-a5d0-605669db524f"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import hashlib\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "# Set Pandas display options\n",
        "pd.set_option('display.max_columns', None)        # Show all columns\n",
        "pd.set_option('display.max_rows', None)           # Show all rows\n",
        "pd.set_option('display.width', None)              # Show full width of the DataFrame\n",
        "pd.set_option('display.max_colwidth', None)       # Show full content of each cell\n",
        "\n",
        "# Basic scientific computing libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Advanced scientific computing and statistics\n",
        "from scipy.io import loadmat\n",
        "from scipy import stats\n",
        "from scipy.stats import mode, skew, kurtosis\n",
        "\n",
        "# Machine learning and data processing\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# PyTorch for deep learning\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "\n",
        "# Other libraries\n",
        "import h5py\n",
        "\n",
        "try:\n",
        "  # Import google.colab to check if running in Colab\n",
        "  import google.colab\n",
        "  from google.colab import runtime\n",
        "  colab_env = True  # Set flag to True for Colab environment\n",
        "except:\n",
        "  # Set flag to False if not in Colab\n",
        "  colab_env = False\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive/')\n",
        "  gdrive_path = '/content/gdrive/MyDrive/'  # Define Google Drive path in Colab\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# LAVA framework\n",
        "try:\n",
        "  import lava.lib.dl.slayer as slayer\n",
        "except:\n",
        "  print('LAVA not included')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb99QxP3S0w6"
      },
      "source": [
        "### Function definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4wjcFN7SM2q"
      },
      "outputs": [],
      "source": [
        "def get_adam_optimizer(learning_rate):\n",
        "  \"\"\"\n",
        "  Creates and returns an Adam optimizer for a globally defined model with a specified learning rate.\n",
        "\n",
        "  Args:\n",
        "  learning_rate (float): The learning rate to be used for the optimizer.\n",
        "\n",
        "  Globals:\n",
        "  model: The globally defined model for which the optimizer is to be created.\n",
        "\n",
        "  Returns:\n",
        "  An instance of torch.optim.Adam configured with the specified learning rate.\n",
        "  \"\"\"\n",
        "  return torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "def hash_dictionary(dictionary):\n",
        "  \"\"\"\n",
        "  Generates a truncated MD5 hash for a given dictionary.\n",
        "\n",
        "  This function serializes the dictionary into a JSON string, ensuring the keys are sorted,\n",
        "  and then computes an MD5 hash of this string. The hash is then truncated to 8 characters.\n",
        "\n",
        "  Parameters:\n",
        "  dictionary: The dictionary to hash. It should be capable of being serialized into JSON.\n",
        "\n",
        "  Returns:\n",
        "  str: An 8-character string representing the truncated MD5 hash of the dictionary.\n",
        "\n",
        "  Example:\n",
        "  >>> hash_dictionary({'key1': 'value1', 'key2': 'value2'})\n",
        "  'e201065d'\n",
        "  \"\"\"\n",
        "\n",
        "  dict_string = json.dumps(dictionary, sort_keys=True)\n",
        "  return hashlib.md5(dict_string.encode()).hexdigest()\n",
        "\n",
        "\n",
        "\n",
        "def find_session_byhash(path, hash):\n",
        "  \"\"\"\n",
        "  Finds the last folder of a specific session based on a provided hash.\n",
        "\n",
        "  This function searches through subdirectories in the specified path, looking for a file named 'session_hash.txt' in each directory. It compares the content of this file, which should be the session's hash, with the provided hash parameter. If a match is found, it returns only the name of the last folder in the path of the corresponding directory.\n",
        "\n",
        "  Parameters:\n",
        "  path (str): The base path to start searching for session directories.\n",
        "  hash (str): The hash of the session to be found.\n",
        "\n",
        "  Returns:\n",
        "  str or None: The name of the last folder in the path of the directory containing the matching session hash, if found; otherwise, None.\n",
        "  \"\"\"\n",
        "\n",
        "  # Iterate through all 'session_parameters.json' files in the specified path\n",
        "  for root, dirs, files in os.walk(path):\n",
        "    for file in files:\n",
        "      if file == 'session_hash.txt':\n",
        "        file_path = os.path.join(root, file)\n",
        "\n",
        "        with open(file_path, 'r') as file:\n",
        "          session_hash = file.read().strip()\n",
        "\n",
        "        if session_hash == hash:\n",
        "          return os.path.basename(root)\n",
        "\n",
        "  return None\n",
        "\n",
        "\n",
        "\n",
        "def save_json(data, file_path):\n",
        "  \"\"\"\n",
        "  Saves a given dictionary as a JSON file at the specified file path.\n",
        "\n",
        "  Args:\n",
        "  - data (dict): The dictionary to be saved as JSON.\n",
        "  - file_path (str): The file path where the JSON file will be saved.\n",
        "\n",
        "  This function takes a dictionary and a file path as inputs and writes the dictionary as a JSON\n",
        "  file at the given path. It handles any exceptions related to file operations and prints\n",
        "  a relevant message in case of an error.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(file_path, 'w') as file:\n",
        "    json.dump(data, file, indent=4)\n",
        "\n",
        "\n",
        "\n",
        "def load_json(file_path):\n",
        "  \"\"\"\n",
        "  Loads data from a JSON file into a Python dictionary.\n",
        "\n",
        "  Args:\n",
        "  - file_path (str): The file path of the JSON file to be loaded.\n",
        "\n",
        "  Returns:\n",
        "  - dict: The data loaded from the JSON file.\n",
        "\n",
        "  This function takes a file path as input and reads the JSON file from the given path into a Python dictionary.\n",
        "  It handles any exceptions related to file operations and prints a relevant message in case of an error.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(file_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "def normalize_json_files(path, dict_name):\n",
        "  \"\"\"\n",
        "  Normalizza i file JSON in una directory specificata e compila i loro contenuti in un DataFrame Pandas.\n",
        "\n",
        "  Questa funzione cerca i file JSON con un nome specificato nel percorso di directory fornito. Normalizza ciascun file JSON in un DataFrame Pandas, estrae il nome dell'hash della sessione dalla cartella in cui ogni file è stato trovato, e compila tutti i dati in un unico DataFrame.\n",
        "\n",
        "  Parametri:\n",
        "  path (str): Il percorso del file dove si trovano i file JSON specificati.\n",
        "  dict_name (str): Il nome dei file JSON da cercare (es., 'session_parameters.json').\n",
        "\n",
        "  Restituisce:\n",
        "  pd.DataFrame: Un DataFrame contenente i dati normalizzati da ciascun file JSON, con la colonna 'session_hash' come prima colonna, indicante l'hash della sessione della cartella dove ogni file JSON è stato trovato.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create an empty DataFrame\n",
        "  aggregated_df = pd.DataFrame()\n",
        "\n",
        "  # Iterate through all 'session_parameters.json' files in the specified path\n",
        "  for root, dirs, files in os.walk(path):\n",
        "    for file in files:\n",
        "      if file == dict_name:\n",
        "        file_path = os.path.join(root, file)\n",
        "\n",
        "        # Read and normalize the JSON file\n",
        "        with open(file_path, 'r') as f:\n",
        "          data = json.load(f)\n",
        "        normalized_data = pd.json_normalize(data)\n",
        "\n",
        "        with open(os.path.join(root, 'session_hash.txt'), 'r') as file:\n",
        "          session_hash = file.read().strip()\n",
        "\n",
        "        # Insert 'FolderName' as the first column\n",
        "        normalized_data.insert(0, 'session_hash', session_hash)\n",
        "\n",
        "        # Add the normalized data to the aggregated DataFrame\n",
        "        aggregated_df = pd.concat([aggregated_df, normalized_data], ignore_index=True, sort=False)\n",
        "\n",
        "  return aggregated_df\n",
        "\n",
        "\n",
        "\n",
        "def print_unique_successive_elements(data_array):\n",
        "  \"\"\"\n",
        "  Prints unique successive elements from a given array.\n",
        "\n",
        "  Args:\n",
        "  - data_array: Array from which unique successive elements are extracted.\n",
        "\n",
        "  This function iterates through the array, identifying and collecting elements that differ\n",
        "  from their immediate predecessors. It then prints the list of these unique elements.\n",
        "  \"\"\"\n",
        "\n",
        "  prev = None\n",
        "  unique_successive_elements = []\n",
        "\n",
        "  for elem in data_array:\n",
        "    elem = int(np.squeeze(elem))  # Convert to standard Python int\n",
        "    if elem != prev:\n",
        "      unique_successive_elements.append(elem)\n",
        "      prev = elem\n",
        "\n",
        "  print(\"Unique successive elements:\", unique_successive_elements)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_single_file(file_path):\n",
        "  \"\"\"\n",
        "  Loads data from a single .mat file and returns EMG, stimulus, and repetition data.\n",
        "\n",
        "  Args:\n",
        "  - file_path: Path to the .mat file to be loaded.\n",
        "\n",
        "  The function extracts 'emg', 'restimulus', and 'rerepetition' arrays from the .mat file.\n",
        "  If a debug flag is set, it also prints additional details about the file contents.\n",
        "\n",
        "  Returns:\n",
        "  - Tuple of Numpy arrays: EMG data, stimulus data, repetition data.\n",
        "  \"\"\"\n",
        "\n",
        "  mat_data = loadmat(file_path)\n",
        "\n",
        "  if False:\n",
        "    print(mat_data.keys())\n",
        "    for key in mat_data.keys():\n",
        "      if key.startswith('__'):\n",
        "          continue\n",
        "      data = mat_data[key]\n",
        "      sample_data = data[:5] if len(data) >= 5 else data\n",
        "      print(f\"Key: {key}, Length: {len(data)}, Sample Data: {sample_data}\")\n",
        "\n",
        "    print_unique_successive_elements(mat_data['stimulus'])\n",
        "    print_unique_successive_elements(mat_data['rerepetition'])\n",
        "\n",
        "  emg = np.array(mat_data['emg'])\n",
        "  stimulus = np.array(mat_data['restimulus'])\n",
        "  repetition = np.array(mat_data['rerepetition'])\n",
        "\n",
        "  return emg, stimulus, repetition\n",
        "\n",
        "\n",
        "\n",
        "def natural_sort_key(s):\n",
        "  \"\"\"\n",
        "  Generates a sorting key for natural (human-friendly) sorting of strings.\n",
        "\n",
        "  Args:\n",
        "  - s: String to be sorted.\n",
        "\n",
        "  The function splits the input string into a list of integers and non-integer substrings,\n",
        "  facilitating natural sorting where numerical parts are sorted numerically.\n",
        "\n",
        "  Returns:\n",
        "  - List of mixed integer and string parts of the input.\n",
        "  \"\"\"\n",
        "\n",
        "  return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
        "\n",
        "\n",
        "\n",
        "def load_all_data(base_folder, selected_subjects, selected_exercises):\n",
        "  \"\"\"\n",
        "  Loads all data from a specified base folder, processing each subject's data\n",
        "  and filtering based on selected exercises.\n",
        "\n",
        "  Args:\n",
        "  - base_folder: Directory containing subject folders.\n",
        "  - selected_subjects: List of selected subjects for data loading.\n",
        "  - selected_exercises: List of selected exercises for data loading.\n",
        "\n",
        "  The function iterates through each subject folder, loading data for predefined exercises\n",
        "  from .mat files. It handles sorting and directory traversal to compile a dictionary\n",
        "  of all subjects' data, organized by subject and exercise.\n",
        "\n",
        "  Returns:\n",
        "  - Dictionary containing data for all subjects, organized by subject and exercise.\n",
        "  \"\"\"\n",
        "\n",
        "  all_data = {}\n",
        "  subjects = os.listdir(base_folder)\n",
        "  subjects = sorted(subjects, key=natural_sort_key)\n",
        "  if '.DS_Store' in subjects:\n",
        "    subjects.remove('.DS_Store')\n",
        "\n",
        "  for s, subject in enumerate(subjects):\n",
        "    if not selected_subjects or s in (selected_subjects):\n",
        "      subject_folder = os.path.join(base_folder, subject)\n",
        "      subfolders = [d for d in os.listdir(subject_folder) if os.path.isdir(os.path.join(subject_folder, d))]\n",
        "      if subfolders:\n",
        "        subject_folder = os.path.join(subject_folder, subfolders[0])\n",
        "\n",
        "      subject_data = {}\n",
        "\n",
        "      exercises = os.listdir(subject_folder) ##\n",
        "      exercises = sorted(exercises, key=natural_sort_key)\n",
        "      if '.DS_Store' in exercises:\n",
        "        exercises.remove('.DS_Store')\n",
        "      for file_name in exercises:\n",
        "        for exercise in selected_exercises:\n",
        "          if exercise in file_name:\n",
        "            file_path = os.path.join(subject_folder, file_name)\n",
        "            emg_data, stimulus_data, repetition_data = load_single_file(file_path)\n",
        "            subject_data[exercise] = {'emg': emg_data, 'stimulus': stimulus_data, 'repetition': repetition_data}\n",
        "\n",
        "      all_data[f'{subject}'] = subject_data\n",
        "\n",
        "  return all_data\n",
        "\n",
        "\n",
        "\n",
        "def update_set_with_list(unique_set, new_list):\n",
        "    \"\"\"\n",
        "    Update a given set with elements from a new list.\n",
        "\n",
        "    This function adds each element from the provided list to the set.\n",
        "    Since a set only holds unique elements, any duplicates in the list\n",
        "    will not be added if they are already present in the set.\n",
        "\n",
        "    Parameters:\n",
        "    unique_set (set): The set to be updated with new elements.\n",
        "    new_list (list): The list of new elements to add to the set.\n",
        "\n",
        "    Returns:\n",
        "    None: The function updates the set in-place and does not return a value.\n",
        "\n",
        "    Example:\n",
        "    >>> my_set = {1, 2, 3}\n",
        "    >>> update_set_with_list(my_set, [3, 4, 5])\n",
        "    >>> print(my_set)\n",
        "    {1, 2, 3, 4, 5}\n",
        "    \"\"\"\n",
        "    for element in new_list:\n",
        "        unique_set.add(element)\n",
        "\n",
        "\n",
        "\n",
        "def process_data(array, max_derivative_order, delta_value, ch_start=None, ch_end=None):\n",
        "  \"\"\"\n",
        "  Processes an array of data by expanding it based on derivative order and generating a spike array.\n",
        "\n",
        "  Args:\n",
        "  - array: Numpy array containing the data to be processed.\n",
        "  - max_derivative_order: Maximum order of derivative to be used in processing.\n",
        "  - delta_value: Delta value used for determining spikes.\n",
        "  - ch_start: (Optional) Starting index for channel slicing.\n",
        "  - ch_end: (Optional) Ending index for channel slicing.\n",
        "\n",
        "  The function first slices the array based on provided channel indices. It then expands the array\n",
        "  to include derivatives up to the specified order. A spike array is generated based on the\n",
        "  delta value, where spikes represent significant changes in the data.\n",
        "\n",
        "  Returns:\n",
        "  - Tuple containing the expanded array and the spike array.\n",
        "  \"\"\"\n",
        "\n",
        "  array = array[:, ch_start:ch_end]\n",
        "  old_dim_size = array.shape[1]\n",
        "  new_dim_size = old_dim_size * (max_derivative_order + 1)\n",
        "\n",
        "  spike_array = np.zeros((array.shape[0] - max_derivative_order * 4, new_dim_size * 2), dtype=np.int8)\n",
        "  expanded_array = np.zeros((array.shape[0], new_dim_size)) #, dtype=np.int16\n",
        "  expanded_array[:, :old_dim_size] = array\n",
        "\n",
        "  for n in range(1, max_derivative_order + 1):\n",
        "    for i in range(old_dim_size):\n",
        "      for j in range(array[:, i].shape[0] - n * 4):\n",
        "        expanded_array[j + n * 2, old_dim_size * n + i] = - expanded_array[j, old_dim_size * (n - 1) + i] - 2 * expanded_array[j + 1, old_dim_size * (n - 1) + i] + 2 * expanded_array[j + 2, old_dim_size * (n - 1) + i] + expanded_array[j + 3, old_dim_size * (n - 1) + i]\n",
        "\n",
        "  # Trim the arrays to the minimum shape\n",
        "  expanded_array = expanded_array[max_derivative_order * 2 : - max_derivative_order * 2, :]\n",
        "\n",
        "  for n in range(max_derivative_order + 1):\n",
        "    for i in range(old_dim_size):\n",
        "      dc_val = expanded_array[0, old_dim_size * n + i] # or = 0\n",
        "      for k, j in enumerate(expanded_array[:, old_dim_size * n + i]):\n",
        "        if j > dc_val + delta_value[n]:\n",
        "          dc_val = j\n",
        "          spike_array[k, (old_dim_size * n + i) * 2] = 1\n",
        "          spike_array[k, (old_dim_size * n + i) * 2 + 1] = 0\n",
        "        elif j < dc_val - delta_value[n]:\n",
        "          dc_val = j\n",
        "          spike_array[k, (old_dim_size * n + i) * 2] = 0\n",
        "          spike_array[k, (old_dim_size * n + i) * 2 + 1] = 1\n",
        "        else:\n",
        "          spike_array[k, (old_dim_size * n + i) * 2] = 0\n",
        "          spike_array[k, (old_dim_size * n + i) * 2 + 1] = 0\n",
        "\n",
        "  return expanded_array, spike_array\n",
        "\n",
        "\n",
        "\n",
        "def plot_emg(data, subject, exercise, figsize=(10, 6), alpha=0.3):\n",
        "  \"\"\"\n",
        "  Plots EMG data for a given subject and exercise with stimulus and repetition annotations.\n",
        "\n",
        "  Args:\n",
        "  - data: Dictionary containing EMG, stimulus, and repetition data.\n",
        "  - subject: The subject identifier whose data is to be plotted.\n",
        "  - exercise: The exercise identifier for the specific data to plot.\n",
        "  - figsize: (Optional) Size of the figure for the plot.\n",
        "  - alpha: (Optional) Transparency level of the plot.\n",
        "\n",
        "  This function visualizes EMG data for the specified subject and exercise. It includes overlay plots\n",
        "  for stimulus and repetition. Stimulus data is represented as colored regions, and repetition data\n",
        "  as a dashed line. The plot includes customization options for size and transparency.\n",
        "\n",
        "  The function handles multiple channels of EMG data, assigns different colors to each, and\n",
        "  represents different stimulus conditions with unique colors. It also sets up the plot with\n",
        "  appropriate labels, legends, and axes.\n",
        "\n",
        "  No return value, as the function's purpose is to display a plot.\n",
        "  \"\"\"\n",
        "\n",
        "  emg_data = data[subject][exercise]['emg'][:,:16]\n",
        "  stimulus = data[subject][exercise]['stimulus'].flatten()\n",
        "  repetition = data[subject][exercise]['repetition'].flatten()\n",
        "\n",
        "  # Extended color list for channels\n",
        "  colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'orange', 'purple', 'pink', 'brown', 'grey', 'lime', 'navy', 'teal', 'maroon']\n",
        "\n",
        "  # Real labels for stimulus\n",
        "  stimulus_real_labels = {\n",
        "      0: 'Rest',\n",
        "      1: 'Index flexion',\n",
        "      2: 'Index extension',\n",
        "      3: 'Middle flexion',\n",
        "      4: 'Middle extension',\n",
        "      5: 'Ring flexion',\n",
        "      6: 'Ring extension',\n",
        "      7: 'Little finger flexion',\n",
        "      8: 'Little finger extension',\n",
        "      9: 'Thumb adduction',\n",
        "      10: 'Thumb abduction',\n",
        "      11: 'Thumb flexion',\n",
        "      12: 'Thumb extension'\n",
        "  }\n",
        "\n",
        "  # Extended color list with real labels for stimulus\n",
        "  stimulus_colors = {\n",
        "      0: 'white',\n",
        "      1: 'green',\n",
        "      2: 'red',\n",
        "      3: 'blue',\n",
        "      4: 'yellow',\n",
        "      5: 'purple',\n",
        "      6: 'orange',\n",
        "      7: 'pink',\n",
        "      8: 'brown',\n",
        "      9: 'grey',\n",
        "      10: 'lime',\n",
        "      11: 'navy',\n",
        "      12: 'teal'\n",
        "  }\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "  # Plot each channel\n",
        "  for i in range(emg_data.shape[1]):\n",
        "      ax.plot(emg_data[:, i], alpha=alpha, color=colors[i])\n",
        "\n",
        "  # Add colored regions for stimulus labels\n",
        "  legend_handles = []\n",
        "  for label, label_name in stimulus_real_labels.items():\n",
        "      color = stimulus_colors[label]\n",
        "      ax.fill_between(np.arange(len(stimulus)), np.min(emg_data), np.max(emg_data),\n",
        "                      where=(stimulus==label), facecolor=color, alpha=alpha)\n",
        "      legend_handles.append(plt.Rectangle((0,0),1,1, color=color, label=label_name))\n",
        "\n",
        "  # Add labels and legend for stimulus labels\n",
        "  ax.set_xlabel('Time (samples)')\n",
        "  ax.set_ylabel('Amplitude')\n",
        "  ax.legend(handles=legend_handles, title='Stimulus Labels')\n",
        "\n",
        "  # Create a secondary axis for 'repetition'\n",
        "  ax2 = ax.twinx()\n",
        "  ax2.plot(repetition, 'k--', label='Repetition', alpha=alpha)\n",
        "  ax2.set_ylabel('Repetition')\n",
        "  ax2.legend(loc='upper right')\n",
        "\n",
        "\n",
        "  # Set x-axis ticks based on the user-specified step\n",
        "  x_step = 1000\n",
        "  x_ticks = np.arange(0, len(stimulus), x_step)\n",
        "  ax.set_xticks(x_ticks)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def get_possible_labels(tensor):\n",
        "  \"\"\"\n",
        "  Extracts unique elements from each row of a tensor and returns them as a list of lists.\n",
        "\n",
        "  Args:\n",
        "  - tensor: A NumPy array or tensor from which unique elements are to be extracted.\n",
        "\n",
        "  This function iterates through each row of the provided tensor. For each row, it finds unique\n",
        "  elements and appends them as a list to `label_lists`. Each sublist in `label_lists` corresponds\n",
        "  to the unique elements found in each row of the tensor.\n",
        "\n",
        "  Returns:\n",
        "  - label_lists: A list of lists, where each sublist contains unique elements from the respective row of the tensor.\n",
        "  \"\"\"\n",
        "\n",
        "  label_lists = []\n",
        "  for i in range(tensor.shape[0]):\n",
        "    label_lists.append(np.unique(tensor[i, :]).tolist())\n",
        "    # if (lenlabel_lists[-1]):\n",
        "  return label_lists\n",
        "\n",
        "\n",
        "\n",
        "def perform_voting(tensor):\n",
        "  \"\"\"\n",
        "  Performs a voting mechanism across rows of a tensor to determine the most frequent element (mode).\n",
        "\n",
        "  Args:\n",
        "  - tensor: A NumPy array or tensor on which voting is to be performed.\n",
        "\n",
        "  This function iterates over each row of the given tensor. For each row, it calculates the mode,\n",
        "  which is the most frequently occurring element in that row. The mode is determined using the\n",
        "  `scipy.stats.mode` function. If the mode result is a scalar, it is directly used; otherwise,\n",
        "  the first element of the mode result is used. This is necessary as `scipy.stats.mode` can return\n",
        "  an array of modes in case of ties.\n",
        "\n",
        "  The function creates an array `winners` to store the mode of each row.\n",
        "\n",
        "  Returns:\n",
        "  - winners: A NumPy array of the same length as the number of rows in `tensor`,\n",
        "    containing the mode of each row.\n",
        "  \"\"\"\n",
        "  winners = np.zeros(tensor.shape[0], dtype=np.int8)\n",
        "  for i in range(tensor.shape[0]):\n",
        "    mode_result = mode(tensor[i, :])\n",
        "    if np.isscalar(mode_result.mode):\n",
        "      winners[i] = mode_result.mode\n",
        "    else:\n",
        "      winners[i] = mode_result.mode[0]\n",
        "  return winners\n",
        "\n",
        "\n",
        "\n",
        "def safe_format(value, fmt=\".5f\"):\n",
        "  \"\"\"\n",
        "  Safely format a given value, handling None cases.\n",
        "\n",
        "  Args:\n",
        "  value: The value to be formatted. Can be None or any value that supports formatting.\n",
        "  fmt: The format string (default is \".5f\").\n",
        "\n",
        "  Returns:\n",
        "  A formatted string according to 'fmt' if 'value' is not None, otherwise \"N/D\" (Not Available).\n",
        "  \"\"\"\n",
        "\n",
        "  return f\"{value:{fmt}}\" if value is not None else \"N/D\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26YMK0AM0rB9"
      },
      "source": [
        "## Load or create session\n",
        "This section of the notebook outlines the setup and initialization process for a machine learning session, focusing on configuring session parameters and managing session results.\n",
        "\n",
        "The only editable variables in the notebook are:\n",
        "- `session_info`: It contains general information about the session, including the project name, session name, and settings related to the handling of results.\n",
        "- `session_parameters`: This details the specific parameters for the session, covering dataset details, neural network configurations, and training settings.\n",
        "- `optimizer_functions`: This dictionary serving as a repository for various optimizer functions.\n",
        "\n",
        "Finally, it is possible to view existing sessions, the analysis focuses on reviewing existing sessions by examining two key files: `session_parameters.json` and `session_results.json`. The process involves normalizing these JSON files into DataFrames, followed by selective filtering to extract relevant information. This approach facilitates a comprehensive understanding of the session's parameters and outcomes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkwhM8TPeB5A"
      },
      "source": [
        "### Session setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HUat3XWeIcU",
        "outputId": "d15c607f-ac6c-46f7-c056-b3c50f701c9c"
      },
      "outputs": [],
      "source": [
        "session_info = {\n",
        "  'project_name' : 'semg-gesturerecognition-snn', # Project name.\n",
        "  'session_loadbyhash' : 'c96a2b5b',                      # Complete this field to load a session using its hash.\n",
        "  'session_note' : '',               # Session note.\n",
        "                                                  # Set the path to the project's output directory\n",
        "  'session_output_path' : os.path.join(gdrive_path, f'Projects/semg-gesturerecognition-snn/output/'),\n",
        "\n",
        "  #'hash_rename' : True,                          # If active, the folder containing the results will be renamed with the hash of the session_parameters dictionary.\n",
        "  'overwrite' : False,                            # Flag to indicate if existing files should be overwritten.\n",
        "  'github_support' : True,                        # Indicates if GitHub support is enabled.\n",
        "  'repository_branch' : None,                     # Branch of the repository to use, default is 'main'.\n",
        "  'lazygit_support' : False,                      # Indicates if LazyGit support is enabled.\n",
        "\n",
        "  'dataset' : {\n",
        "    'dataset_path' : 'dataset/',                  # Path to the dataset directory.\n",
        "    'ninapro_db2_download' : False,               # Downloading the NinaPro DB2 dataset.\n",
        "    'plot_exampledata' : False                    # Flag to indicate if example data from the dataset should be plotted.\n",
        "  },\n",
        "\n",
        "  'training' : {\n",
        "    'epoch_max' : None,                           # Maximum number of training epochs. If set to None, there will be no precise number of maximum epochs.\n",
        "    'bestmodel_lrchange' : True,                  # If set to True, the model will revert to the best model state when a learning rate change occurs.\n",
        "    'auto_unassign' : False,                      # Flag to indicate automatic unassignment of resources post-training.\n",
        "  }\n",
        "}\n",
        "\n",
        "session_parameters = {\n",
        "  'dataset' : {\n",
        "    'dataset_subpath' : 'ninapro/db5/',           # Subpath to the dataset directory. Currently tested {'ninapro/db1/', 'ninapro/db2/', 'ninapro/db5/'}.\n",
        "    'data_freq_hz' : 200,                         # Frequency of data sampling in Hertz.\n",
        "    'subjects' : [],                              # List of subject IDs (enumarate) to include in the dataset. Set an empty list to include all subjects.\n",
        "    'exercises' : ['E1'],                         # List of exercise IDs to include in the dataset. Specify specific exercises or set an empty list to include all exercises.\n",
        "    'channel_start' : None,                       # Starting index of channels to consider (None for the first possible channel).\n",
        "    'channel_end' : None,                         # Ending index of channels to consider (None for the last possible channel).\n",
        "    'max_derivative_order' : 2,                   # Maximum order of derivative to calculate for the data.\n",
        "    'delta_value' : 15,                           # Value of delta for delta encoding. Set a list of values if you want different values per layer.\n",
        "\n",
        "    'train_repetition' : [1, 4, 6],               # Repetitions to be used for training.\n",
        "    'valid_repetition' : [3],                     # Repetitions to be used for validation.\n",
        "    'test_repetition' : [2, 5],                   # Repetitions to be used for testing.\n",
        "\n",
        "    'exercise_win_ratio' : 0.5,                   # Min ratio of the window to be a specific repetition for consideration in exercise.\n",
        "    'exercise_min_ratio' : 0.8,                   # Min ratio of 'exercise' labels in a window to be considered an exercise.\n",
        "    'rest_min_ratio' : 1,                         # Min ratio of 'rest' labels in a window to be considered rest.\n",
        "\n",
        "    'win_size_s' : 0.5,                           # Size of each window in seconds.\n",
        "    'win_shift_s' : 0.1,                          # Shift of the sliding window in seconds.\n",
        "    'start_delay_s' : 2,                          # Delay in seconds before starting data collection.\n",
        "    'rest_delay_s' : 0,                           # Delay in seconds after each exercise session before starting data collection.\n",
        "\n",
        "    'random_seed' : 0,                            # Seed for random number generation to ensure reproducibility.\n",
        "  },\n",
        "\n",
        "  'network' : {\n",
        "    'current_decay' : 1,                          # Decay rate of the current; typical values are 1 for LIF and 0.25 for CUBA neuron models.\n",
        "    'ofs' : [64, 128, 64, None],                  # Output feature values for network layers, the last feature output value (number of classes) is calculated automatically if set to None.\n",
        "    'max_delay' : 62                              # Maximum delay allowed in the network's synaptic connections. Set a list of values if you want different values per layer.\n",
        "  },\n",
        "\n",
        "  'training' : {\n",
        "    'pretrained_model' : None,                    # Set a pretrained-folder path if needed.\n",
        "    'optimizer' : 'adam',                         # Optimizer to use for training, currently supported {'adam'}.\n",
        "    'best_loss' : True,                           # If set to True, the best model will be selected during training based on the lowest loss on the validation set. If set to False, the selection will be based on the highest accuracy.\n",
        "    'batch_size' : 32,                            # Batch size for training.\n",
        "    'true_rate' : 0.2,                            # The firing rate set for positive class outputs during training. This rate is used to define the desired level of neuron activity for correct classifications.\n",
        "    'false_rate' : 0.03,                          # The firing rate set for negative class outputs during training. This rate determines the neuron activity level for incorrect classifications.\n",
        "    'learning_rate_first' : 0.001,                # Initial learning rate for the optimizer.\n",
        "    'learning_rate_decay' : 0.1,                  # Factor by which the learning rate decays.\n",
        "    'learning_rate_decay_steps' : 3,              # Number of times the learning_rate_decay is applied.\n",
        "    'patience' : 30                               # Number of epochs to wait for improvement before stopping training or changing the learning rate.\n",
        "  }\n",
        "}\n",
        "\n",
        "session_results = {\n",
        "  'hash_name' : None,                # UNEDITABLE # Hash of the session_parameters dictionary.\n",
        "  'colab_env' : None,                # UNEDITABLE # Google Colab support.\n",
        "  'device' : None,                   # UNEDITABLE # Hardware support.\n",
        "\n",
        "  'network': {\n",
        "    'ofs': None                      # UNEDITABLE # Final offset values determined for the network layers.\n",
        "  }\n",
        "}\n",
        "\n",
        "# Dictionary serving as a repository for various optimizer functions.\n",
        "optimizer_functions = {\n",
        "  'adam': get_adam_optimizer\n",
        "}\n",
        "\n",
        "if not isinstance(session_parameters['dataset']['delta_value'], list):\n",
        "  session_parameters['dataset']['delta_value'] = [session_parameters['dataset']['delta_value']] * (session_parameters['dataset']['max_derivative_order'] + 1) # len(session_parameters['network']['ofs'])\n",
        "\n",
        "if not isinstance(session_parameters['network']['max_delay'], list):\n",
        "  session_parameters['network']['max_delay'] = [session_parameters['network']['max_delay']] * len(session_parameters['network']['ofs'])\n",
        "  session_parameters['network']['max_delay'][-1] = None\n",
        "\n",
        "session_results['hash_name'] = hash_dictionary(session_parameters)[:8]\n",
        "session_results['colab_env'] = colab_env\n",
        "session_results['device'] = str(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "session_folder_name = session_results['hash_name'] # if session_info['hash_rename'] else session_info['session_name']\n",
        "session_loaded = False\n",
        "\n",
        "if session_info['overwrite']:\n",
        "  try:\n",
        "    shutil.rmtree(os.path.join(session_info['session_output_path'], session_folder_name))\n",
        "  except:\n",
        "    pass\n",
        "  os.makedirs(os.path.join(session_info['session_output_path'], session_folder_name), exist_ok=True)\n",
        "\n",
        "  print('New session overwritten:', os.path.join(session_info['session_output_path'], session_folder_name))\n",
        "\n",
        "else:\n",
        "  for session_hash in [session_info['session_loadbyhash']] if session_info['session_loadbyhash'] else [session_results['hash_name']]:\n",
        "    existing_session_folder_name = find_session_byhash(session_info['session_output_path'], session_hash)\n",
        "    if existing_session_folder_name:\n",
        "      try:\n",
        "        session_info_loaded = load_json(os.path.join(session_info['session_output_path'], existing_session_folder_name, 'session_info.json'))\n",
        "        session_parameters_loaded = load_json(os.path.join(session_info['session_output_path'], existing_session_folder_name, 'session_parameters.json'))\n",
        "        session_results_loaded = load_json(os.path.join(session_info['session_output_path'], existing_session_folder_name, 'session_results.json'))\n",
        "\n",
        "        session_info_loaded['overwrite'] = session_info['overwrite']\n",
        "        session_info_loaded['lazygit_support'] = session_info['lazygit_support']\n",
        "        session_info_loaded['dataset']['plot_exampledata'] = session_info['dataset']['plot_exampledata']\n",
        "        session_info_loaded['training']['epoch_max'] = session_info['training']['epoch_max']\n",
        "        session_info_loaded['training']['auto_unassign'] = session_info['training']['auto_unassign']\n",
        "\n",
        "        session_results_loaded['hash_name'] = session_results['hash_name']\n",
        "        session_results_loaded['colab_env'] = session_results['colab_env']\n",
        "        session_results_loaded['device'] = session_results['device']\n",
        "\n",
        "        session_info = session_info_loaded\n",
        "        session_parameters = session_parameters_loaded\n",
        "        session_results = session_results_loaded\n",
        "\n",
        "        session_folder_name = existing_session_folder_name\n",
        "\n",
        "        session_loaded = True\n",
        "        print('Existing session loaded:', os.path.join(session_info['session_output_path'], existing_session_folder_name))\n",
        "        break\n",
        "      except:\n",
        "        print('Error loading session with hash:', session_info['session_loadbyhash'])\n",
        "        raise\n",
        "\n",
        "if not session_loaded:\n",
        "  if session_info['session_loadbyhash']:\n",
        "    print('No session found with hash:', session_info['session_loadbyhash'])\n",
        "    raise\n",
        "\n",
        "  else:\n",
        "    os.makedirs(os.path.join(session_info['session_output_path'], session_folder_name), exist_ok=True)\n",
        "    print('New session created:', os.path.join(session_info['session_output_path'], session_folder_name))\n",
        "\n",
        "    save_json(session_info, os.path.join(session_info['session_output_path'], session_folder_name, 'session_info.json'))\n",
        "    save_json(session_parameters, os.path.join(session_info['session_output_path'], session_folder_name, 'session_parameters.json'))\n",
        "    save_json(session_results, os.path.join(session_info['session_output_path'], session_folder_name, 'session_results.json'))\n",
        "\n",
        "    with open(os.path.join(session_info['session_output_path'], session_folder_name, 'session_hash.txt'), 'w') as file:\n",
        "      file.write(session_results['hash_name'])\n",
        "\n",
        "try:\n",
        "  if session_results['training']['epoch_last']:\n",
        "    session_trained = True\n",
        "  else:\n",
        "    session_trained = False\n",
        "except:\n",
        "  session_trained = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdF64NoC0c6H"
      },
      "source": [
        "### Existing sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "F-S0cU7jmBoM",
        "outputId": "d7afe626-91d4-4181-b49d-b06e0b279309"
      },
      "outputs": [],
      "source": [
        "# Normalize the JSON files and create a DataFrame\n",
        "df = normalize_json_files(session_info['session_output_path'], 'session_parameters.json')\n",
        "\n",
        "# List of columns to ignore\n",
        "columns_to_ignore = []\n",
        "\n",
        "# Select all columns except those to ignore\n",
        "columns_to_keep = [col for col in df.columns if col not in columns_to_ignore]\n",
        "df_filtered = df[columns_to_keep]\n",
        "\n",
        "# Display the filtered DataFrame\n",
        "pd.DataFrame(df_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "G2Cj4uKy0n_U",
        "outputId": "8d00adbd-926d-4c06-811e-8951d3ecdadd"
      },
      "outputs": [],
      "source": [
        "# Normalize the JSON files and create a DataFrame\n",
        "df = normalize_json_files(session_info['session_output_path'], 'session_results.json')\n",
        "\n",
        "# List of columns to ignore\n",
        "columns_to_ignore = ['hash_name', 'colab_env', 'device', 'training.optimizer', 'training.train_loss', 'training.valid_loss', 'training.train_acc', 'training.valid_acc']\n",
        "\n",
        "# Select all columns except those to ignore\n",
        "columns_to_keep = [col for col in df.columns if col not in columns_to_ignore]\n",
        "df_filtered = df[columns_to_keep]\n",
        "\n",
        "# Display the filtered DataFrame\n",
        "pd.DataFrame(df_filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkdXY7maMYB2"
      },
      "source": [
        "## Session environment\n",
        "This cell is designed for setting up the development environment and repository access, particularly for sessions using GitHub and Colab.\n",
        "\n",
        "- **Google Colab User Data:** Imports user data from Google Colab for authentication purposes.\n",
        "- **Repository Cloning:** If `session_info['github_support']` is true and the session is run in a Colab environment (`session_results['colab_env']`).\n",
        "\n",
        "- **Generic Setup:** For sessions not using Colab or GitHub, an `else` block is included to organize any alternative setup required.\n",
        "\n",
        "- **LazyGit Installation:** If `session_info['lazygit_support']` is true, the cell automates the installation of LazyGit, a simple terminal UI for Git commands. It fetches the latest version, installs it, and cleans up the installation files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzFETAtCJ--v",
        "outputId": "c0d478a8-6ff9-43e6-cba5-aeb0dd0ff093"
      },
      "outputs": [],
      "source": [
        "if session_info['github_support'] and session_results['colab_env']:\n",
        "  # Import userdata module from google.colab\n",
        "  from google.colab import userdata\n",
        "\n",
        "  # Remove the default sample_data directory in Colab\n",
        "  %rm -rf /content/sample_data\n",
        "\n",
        "  # Change current working directory to /content\n",
        "  %cd /content\n",
        "\n",
        "  # Repository name on GitHub\n",
        "  repository = session_info['project_name']\n",
        "\n",
        "  # Retrieve Git credentials stored in userdata\n",
        "  github_json = userdata.get('github_json')\n",
        "  github_json = json.loads(github_json)\n",
        "\n",
        "  # Configure global Git settings with the retrieved credentials\n",
        "  !git config --global user.name {github_json['name']}\n",
        "  !git config --global user.email {github_json['email']}\n",
        "  !git config --global user.password {github_json['password']}\n",
        "\n",
        "  # Clone the GitHub repository using Git token for authentication\n",
        "  !git clone -b {session_info['repository_branch'] if session_info['repository_branch'] else 'main'} https://{github_json['token']}@github.com/{github_json['name']}/{repository}\n",
        "\n",
        "  # Change directory to the cloned repository's directory\n",
        "  %cd /content/{repository}\n",
        "\n",
        "else:\n",
        "  # Change directory to main directory\n",
        "  #%cd ...\n",
        "  pass\n",
        "\n",
        "if session_info['lazygit_support']:\n",
        "  LAZYGIT_VERSION = !echo $(curl -s \"https://api.github.com/repos/jesseduffield/lazygit/releases/latest\" | grep -Po '\"tag_name\": \"v\\K[^\"]*')\n",
        "  LAZYGIT_SOURCE = f\"https://github.com/jesseduffield/lazygit/releases/latest/download/lazygit_{LAZYGIT_VERSION[0]}_Linux_x86_64.tar.gz\"\n",
        "  !curl -Lo lazygit.tar.gz {LAZYGIT_SOURCE}\n",
        "  !tar xf lazygit.tar.gz lazygit\n",
        "  !install lazygit /usr/local/bin\n",
        "  %rm lazygit.tar.gz\n",
        "  %rm lazygit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4npyn6MWA7r-"
      },
      "source": [
        "### Downloading NinaPro DB2\n",
        "If `ninapro_db2_download`, his cell performs the download of the NinaPro DB2 dataset. The dataset is downloaded as a compressed file and then extracted to the specified location for further processing and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIwaLJEe8bvn"
      },
      "outputs": [],
      "source": [
        "if session_results['colab_env'] and session_info['dataset']['ninapro_db2_download']:\n",
        "  # Retrieve Kaggle token from userdata and parse the JSON\n",
        "  kaggle_json = userdata.get('kaggle_json')\n",
        "  kaggle_json = json.loads(kaggle_json)\n",
        "\n",
        "  # Create a kaggle.json file with the token\n",
        "  with open('kaggle.json', 'w') as file:\n",
        "      json.dump(kaggle_json, file, indent=4)\n",
        "\n",
        "  # Print out the details of the kaggle.json file\n",
        "  for fn in kaggle_token.keys():\n",
        "      print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "          name=fn, length=len(kaggle_token[fn])))\n",
        "\n",
        "  # Set up Kaggle API credentials file and permissions\n",
        "  !mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "  # Download the specified Kaggle dataset\n",
        "  !kaggle datasets download \"mengliumei/ninaprodb2\"\n",
        "\n",
        "  # Clean up by removing the kaggle.json file\n",
        "  %rm kaggle.json\n",
        "\n",
        "  # Create a directory for the dataset and unzip the downloaded dataset into it\n",
        "  !mkdir dataset/ninapro/db2\n",
        "  !unzip ninaprodb2.zip -d dataset/ninapro/db2\n",
        "\n",
        "  # Remove the downloaded zip file after extraction\n",
        "  %rm ninaprodb2.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBFOSecEZN0r"
      },
      "source": [
        "## Dataset Processing\n",
        "\n",
        "The code is designed to adaptively process input data from the NinaPro dataset according to various parameters:\n",
        "\n",
        "- `dataset_subpath`: Specifies the particular subpath within the dataset directory.\n",
        "- `data_freq_hz`: Sets the data sampling frequency.\n",
        "- `subjects`: Determines the subject IDs to include in the dataset.\n",
        "- `exercises`: Identifies specific exercises for focused analysis.\n",
        "- `channel_start`, `channel_end`: Defines the range of channels to consider in the dataset.\n",
        "- `max_derivative_order`: Establishes the highest order of derivatives to be calculated.\n",
        "- `delta_value`: Determines the threshold for generating a spike array, highlighting significant data changes.\n",
        "- `plot_exampledata`: Optionally activates plotting of EMG data for selected sessions and exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSc_SnGlTAyY",
        "outputId": "7c78fd72-0789-4e49-dbdd-1a15b4fb8789"
      },
      "outputs": [],
      "source": [
        "all_data = load_all_data(os.path.join(session_info['dataset']['dataset_path'], session_parameters['dataset']['dataset_subpath']), session_parameters['dataset']['subjects'], session_parameters['dataset']['exercises'])\n",
        "\n",
        "# Define the max_derivative_order variable\n",
        "max_derivative_order = session_parameters['dataset']['max_derivative_order']\n",
        "delta_value = session_parameters['dataset']['delta_value']\n",
        "\n",
        "# Original shapes\n",
        "print('EMG signal data samples:')\n",
        "print({subject: {exercise: data['emg'].shape for exercise, data in subject_data.items()} for subject, subject_data in all_data.items()}, '\\n')\n",
        "\n",
        "# Expand 'emg' arrays in 'all_data' based on max_derivative_order\n",
        "for i, (subject, subject_data) in tqdm(enumerate(all_data.items()), ncols=80, desc='Data subject loading', total=len(all_data)):\n",
        "  for exercise, exercise_data in subject_data.items():\n",
        "    all_data[subject][exercise]['emg'], all_data[subject][exercise]['emg_spike'] = process_data(exercise_data['emg'], session_parameters['dataset']['max_derivative_order'], session_parameters['dataset']['delta_value'], ch_start=session_parameters['dataset']['channel_start'], ch_end=session_parameters['dataset']['channel_end'])\n",
        "\n",
        "# Verify the shapes after expansion\n",
        "print('\\n\\nEMG signal and derivative data samples:')\n",
        "print({subject: {exercise: data['emg'].shape for exercise, data in subject_data.items()} for subject, subject_data in all_data.items()})\n",
        "\n",
        "# Verify the shapes after delta modulation process\n",
        "print('\\nEMG spiking data samples:')\n",
        "print({subject: {exercise: data['emg_spike'].shape for exercise, data in subject_data.items()} for subject, subject_data in all_data.items()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96
        },
        "id": "XWNxWoiSWG3Z",
        "outputId": "dc070a56-cea0-42cf-ce93-ced4468180e6"
      },
      "outputs": [],
      "source": [
        "if session_info['dataset']['plot_exampledata'] or True:\n",
        "  plot_emg(all_data, 's1', 'E1', figsize=(180, 8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0OM1P-uZSZ6"
      },
      "source": [
        "## Data Loader\n",
        "\n",
        "The code is configured to effectively handle data loading and preparation across various stages of machine learning model development. The primary settings include:\n",
        "\n",
        "- `train_repetition`, `valid_repetition`, `test_repetition`: Designates separate repetitions for training, validation, and testing phases to ensure distinct data segmentation.\n",
        "- `exercise_win_ratio`, `exercise_min_ratio`, `rest_min_ratio`: Establishes parameters for window analysis, such as the minimum ratio of a window for a specific repetition, and minimum proportions of exercise and rest labels within a window.\n",
        "- `win_size_s`, `win_shift_s`: Defines the size and shift in seconds of the window for detailed data sampling.\n",
        "- `start_delay_s`, `rest_delay_s`: Implements start and post-exercise session delays in seconds for data collection stabilization.\n",
        "- `random_seed`: Sets a random seed to ensure reproducibility of results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jwq32_ZXD5V"
      },
      "outputs": [],
      "source": [
        "# Define which repetition numbers are used for training, validation, and testing\n",
        "train_repetition = session_parameters['dataset']['train_repetition']\n",
        "valid_repetition = session_parameters['dataset']['valid_repetition']\n",
        "test_repetition = session_parameters['dataset']['test_repetition']\n",
        "\n",
        "# Define the minimum ratios for considering a window as exercise or rest\n",
        "exercise_win_ratio = session_parameters['dataset']['exercise_win_ratio']  # Minimum ratio of the window that must be a specific repetition for it to be considered\n",
        "exercise_min_ratio = session_parameters['dataset']['exercise_min_ratio']  # Minimum ratio of 'exercise' labels in the window for it to be considered exercise\n",
        "rest_min_ratio = session_parameters['dataset']['rest_min_ratio']          # Minimum ratio of 'rest' labels in the window for it to be considered rest\n",
        "\n",
        "# Define the frequency of the data\n",
        "data_freq_hz = session_parameters['dataset']['data_freq_hz']\n",
        "\n",
        "# Define the window size, window shift, and delays in seconds\n",
        "win_size_s = session_parameters['dataset']['win_size_s']        # Size of each window in seconds\n",
        "win_shift_s = session_parameters['dataset']['win_shift_s']      # Shift for the sliding window in seconds\n",
        "start_delay_s = session_parameters['dataset']['start_delay_s']  # Initial delay in seconds before starting the windowing\n",
        "rest_delay_s = session_parameters['dataset']['rest_delay_s']    # Delay in seconds after each exercise before considering rest\n",
        "\n",
        "# Convert the window size, window shift, and delays into samples\n",
        "win_size_sample = int(win_size_s * data_freq_hz)        # Window size in samples\n",
        "win_shift_sample = int(win_shift_s * data_freq_hz)      # Window shift in samples\n",
        "start_delay_sample = int(start_delay_s * data_freq_hz)  # Initial delay in samples\n",
        "rest_delay_sample = int(rest_delay_s * data_freq_hz)    # Delay after exercise in samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC_uvuqWYgwe",
        "outputId": "ec04ccd5-850b-4fbc-ad58-b015e5d85edc"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty dictionary to store classes found in the dataset\n",
        "classes = set()\n",
        "\n",
        "# Initialize empty lists to collect data for training, validation, and testing sets\n",
        "x_train_orig = []\n",
        "x_valid_orig = []\n",
        "x_test_orig = []\n",
        "\n",
        "x_train = []\n",
        "x_valid = []\n",
        "x_test = []\n",
        "\n",
        "y_train = []\n",
        "y_valid = []\n",
        "y_test = []\n",
        "\n",
        "# Loop over all subjects and exercises in the all_data dictionary\n",
        "#for subject, subject_data in all_data.items():\n",
        "for i, (subject, subject_data) in tqdm(enumerate(all_data.items()), ncols=80, desc='Data subject loading', total=len(all_data)):\n",
        "  for exercise, exercise_data in subject_data.items():\n",
        "    emg_data_orig = exercise_data['emg']\n",
        "    emg_data = exercise_data['emg_spike']\n",
        "    stimulus = exercise_data['stimulus'].flatten()\n",
        "    repetition = exercise_data['repetition'].flatten()\n",
        "\n",
        "    update_set_with_list(classes, stimulus)\n",
        "\n",
        "    # Initialize starting index for windowing\n",
        "    start_idx = start_delay_sample\n",
        "    exercise_idx = - rest_delay_sample\n",
        "\n",
        "    while start_idx + win_size_sample <= len(emg_data):\n",
        "      # Extract the window from the EMG data, stimulus, and repetition\n",
        "      win_emg_orig = emg_data_orig[start_idx:start_idx + win_size_sample, :]\n",
        "      win_emg = emg_data[start_idx:start_idx + win_size_sample, :]\n",
        "      win_stimulus = stimulus[start_idx:start_idx + win_size_sample]\n",
        "      win_repetition = repetition[start_idx:start_idx + win_size_sample]\n",
        "\n",
        "      # Calculate ratios for the current window\n",
        "      exercise_ratio = np.sum(win_stimulus != 0) / win_size_sample\n",
        "      rest_ratio = np.sum(win_stimulus == 0) / win_size_sample\n",
        "\n",
        "      # Check if the window is mostly one of the training repetitions\n",
        "      is_train_repetition = np.sum(np.isin(win_repetition, train_repetition)) >= win_size_sample * exercise_win_ratio\n",
        "      is_valid_repetition = np.sum(np.isin(win_repetition, valid_repetition)) >= win_size_sample * exercise_win_ratio\n",
        "      is_test_repetition = np.sum(np.isin(win_repetition, test_repetition)) >= win_size_sample * exercise_win_ratio\n",
        "\n",
        "      # Categorize the window based on its characteristics\n",
        "      if is_train_repetition and exercise_ratio >= exercise_min_ratio and start_idx % 20 == 0:\n",
        "        x_train_orig.append(win_emg_orig)\n",
        "        x_train.append(win_emg)\n",
        "        y_train.append(win_stimulus)\n",
        "        exercise_idx = start_idx\n",
        "\n",
        "      elif is_train_repetition and rest_ratio >= rest_min_ratio and start_idx % 20 == 0:\n",
        "        if exercise_idx + win_size_sample + rest_delay_sample <= start_idx:\n",
        "          x_train_orig.append(win_emg_orig)\n",
        "          x_train.append(win_emg)\n",
        "          y_train.append(win_stimulus)\n",
        "\n",
        "      elif is_valid_repetition and exercise_ratio >= exercise_min_ratio and start_idx % 20 == 0:\n",
        "        x_valid_orig.append(win_emg_orig)\n",
        "        x_valid.append(win_emg)\n",
        "        y_valid.append(win_stimulus)\n",
        "        exercise_idx = start_idx\n",
        "\n",
        "      elif is_valid_repetition and rest_ratio >= rest_min_ratio and start_idx % 20 == 0:\n",
        "        if exercise_idx + win_size_sample + rest_delay_sample <= start_idx:\n",
        "          x_valid_orig.append(win_emg_orig)\n",
        "          x_valid.append(win_emg)\n",
        "          y_valid.append(win_stimulus)\n",
        "\n",
        "      elif is_test_repetition and exercise_ratio >= exercise_min_ratio:\n",
        "        x_test_orig.append(win_emg_orig)\n",
        "        x_test.append(win_emg)\n",
        "        y_test.append(win_stimulus)\n",
        "        exercise_idx = start_idx\n",
        "\n",
        "      elif is_test_repetition and rest_ratio >= rest_min_ratio:\n",
        "        if exercise_idx + win_size_sample + rest_delay_sample <= start_idx:\n",
        "          x_test_orig.append(win_emg_orig)\n",
        "          x_test.append(win_emg)\n",
        "          y_test.append(win_stimulus)\n",
        "\n",
        "      # Shift the starting index for the next window\n",
        "      start_idx += win_shift_sample\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "x_train_orig = np.array(x_train_orig)\n",
        "x_valid_orig = np.array(x_valid_orig)\n",
        "x_test_orig = np.array(x_test_orig)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "x_train = np.array(x_train)\n",
        "x_valid = np.array(x_valid)\n",
        "x_test = np.array(x_test)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_valid = np.array(y_valid)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Transpose the arrays\n",
        "x_train_orig = np.transpose(x_train_orig, (0, 2, 1))\n",
        "x_valid_orig = np.transpose(x_valid_orig, (0, 2, 1))\n",
        "x_test_orig = np.transpose(x_test_orig, (0, 2, 1))\n",
        "\n",
        "# Transpose the arrays\n",
        "x_train = np.transpose(x_train, (0, 2, 1))\n",
        "x_valid = np.transpose(x_valid, (0, 2, 1))\n",
        "x_test = np.transpose(x_test, (0, 2, 1))\n",
        "\n",
        "# Check shapes of resulting datasets\n",
        "x_train.shape, x_valid.shape, x_test.shape, y_train.shape, y_valid.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hyq9NurINVsy",
        "outputId": "42bbd1c3-f3ed-4b94-e224-304f9a1b044e"
      },
      "outputs": [],
      "source": [
        "x_test_orig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGdhj6DAYuM1",
        "outputId": "b977ef4f-40a9-4416-b9a7-88c97697f939"
      },
      "outputs": [],
      "source": [
        "# Apply the function to y_train, y_valid, and y_test\n",
        "y_train_multi = get_possible_labels(y_train)\n",
        "y_valid_multi = get_possible_labels(y_valid)\n",
        "y_test_multi = get_possible_labels(y_test)\n",
        "\n",
        "# Apply the function to y_train, y_valid, and y_test\n",
        "y_train = perform_voting(y_train)\n",
        "y_valid = perform_voting(y_valid)\n",
        "y_test = perform_voting(y_test)\n",
        "\n",
        "# Check shapes of resulting datasets\n",
        "x_train.shape, x_valid.shape, x_test.shape, y_train.shape, y_valid.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NlwGf-CZZBD"
      },
      "outputs": [],
      "source": [
        "class EMG_Dataset(Dataset):\n",
        "  \"\"\"\n",
        "  A custom dataset class for Electromyography (EMG) data compatible with PyTorch's Dataset interface.\n",
        "\n",
        "  This class is designed to handle datasets for EMG analysis tasks, providing necessary methods\n",
        "  to integrate with PyTorch's data loading utilities.\n",
        "\n",
        "  Args:\n",
        "  - x: Input data (features), typically EMG signals in a NumPy array or similar format.\n",
        "  - y: Target data (labels), usually in a NumPy array format.\n",
        "\n",
        "  Methods:\n",
        "  - __init__: Initializes the dataset with input and target data.\n",
        "  - __getitem__: Retrieves a single sample (input-target pair) from the dataset at the specified index.\n",
        "  - __len__: Returns the total number of samples in the dataset.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, x, y):\n",
        "    \"\"\"\n",
        "    Initializes the EMG_Dataset instance with input and target data.\n",
        "    \"\"\"\n",
        "\n",
        "    super(EMG_Dataset, self).__init__()\n",
        "    self.input = x\n",
        "    self.target = y\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    Retrieves the input-target pair at the specified index in the dataset.\n",
        "\n",
        "    Args:\n",
        "    - idx: Index of the sample to retrieve.\n",
        "\n",
        "    Returns:\n",
        "    - A tuple containing the input data (as a float32 tensor) and the target label (as a long tensor) for the specified index.\n",
        "    \"\"\"\n",
        "\n",
        "    y_out = torch.tensor(self.target[idx]).long()  # Convert to long tensor\n",
        "    return (\n",
        "      torch.tensor(self.input[idx].astype(np.float32)),  # Convert input to float32 tensor\n",
        "      y_out\n",
        "    )\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Returns the total number of samples in the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    return len(self.target)  # Return the length of the target data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfoJBwApZUJn",
        "outputId": "b951600e-2821-47b0-d513-668d2c4afda6"
      },
      "outputs": [],
      "source": [
        "# Set a random seed for reproducibility of shuffling\n",
        "random_seed = session_parameters['dataset']['random_seed']\n",
        "\n",
        "# Shuffle the training, validation, and testset dataset, both features and labels, using the specified random seed\n",
        "x_train_orig, _ = shuffle(x_train_orig, y_train, random_state=random_seed)\n",
        "x_train, y_train = shuffle(x_train, y_train, random_state=random_seed)\n",
        "#x_valid, y_valid = shuffle(x_valid, y_valid, random_state=random_seed)\n",
        "#x_test, y_test = shuffle(x_test, y_test, random_state=random_seed)\n",
        "\n",
        "if False:\n",
        "  # Convert the shuffled numpy arrays to PyTorch tensors with dtype=torch.int8\n",
        "  x_train_tensor = torch.tensor(x_train, dtype=torch.int8)\n",
        "  y_train_tensor = torch.tensor(y_train, dtype=torch.int8)\n",
        "  x_valid_tensor = torch.tensor(x_valid, dtype=torch.int8)\n",
        "  y_valid_tensor = torch.tensor(y_valid, dtype=torch.int8)\n",
        "  x_test_tensor = torch.tensor(x_test, dtype=torch.int8)\n",
        "  y_test_tensor = torch.tensor(y_test, dtype=torch.int8)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = EMG_Dataset(x_train, y_train)\n",
        "valid_dataset = EMG_Dataset(x_valid, y_valid)\n",
        "test_dataset = EMG_Dataset(x_test, y_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = session_parameters['training']['batch_size']\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "debugtest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Print out some DataLoader details to confirm\n",
        "len(train_dataloader), len(valid_dataloader), len(test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RowmuRBaOdy"
      },
      "source": [
        "## Network\n",
        "\n",
        "The implementation of the custom neural network module in PyTorch is designed for spiking neural network applications, focusing on key features and parameters:\n",
        "\n",
        "- `current_decay`: Configurable decay rate of the current, pivotal in defining the dynamics of spiking neurons. The value is adaptable to different neuron models, like LIF and CUBA.\n",
        "- `ofs`: The network layers are dynamically built based on the output feature values. These values determine the architecture of each layer in the network.\n",
        "- `max_delay`: The maximum delay allowed in the network's synaptic connections, crucial for managing the timing of spike propagation.\n",
        "\n",
        "After initial setup, the code includes logic to manage model loading and training control based on the session's state:\n",
        "\n",
        "- Model Loading: The code determines whether to load an existing model or create a new one based on the session's training status. It handles scenarios of both early stopping and ongoing training.\n",
        "- Training Enablement: A check is performed to decide if further training is necessary, setting the `training_enable` flag accordingly. This decision is based on the availability and status of the pre-trained or last saved model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "metvPEkbaR8a",
        "outputId": "5b769a8a-2bfa-49a7-e82b-94276e7f2913"
      },
      "outputs": [],
      "source": [
        "class Network(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  A custom neural network module implemented using PyTorch, designed for processing spike-based data.\n",
        "\n",
        "  This network consists of multiple dense layers with specific neuron parameters, including dropout,\n",
        "  threshold, current decay, and voltage decay. It is designed for spiking neural network (SNN) applications.\n",
        "\n",
        "  Attributes:\n",
        "  - blocks (torch.nn.ModuleList): A list of dense layers constituting the network.\n",
        "\n",
        "  Methods:\n",
        "  - forward: Defines the forward pass of the network.\n",
        "  - grad_flow: Monitors and visualizes the gradient flow in the network.\n",
        "  - export_hdf5: Exports the network configuration and parameters to an HDF5 file.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initializes the Network instance, setting up the layers and neuron parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    super(Network, self).__init__()\n",
        "\n",
        "    neuron_params = {\n",
        "        'threshold'     : 1.25,\n",
        "        'current_decay' : session_parameters['network']['current_decay'],\n",
        "        'voltage_decay' : 0.03,\n",
        "        'tau_grad'      : 0.03,\n",
        "        'scale_grad'    : 3,\n",
        "        'requires_grad' : True,\n",
        "      }\n",
        "    neuron_params_drop = {**neuron_params, 'dropout' : slayer.neuron.Dropout(p=0.05),}\n",
        "\n",
        "    neuron_params_1 = {\n",
        "        'threshold'     : 1.25,\n",
        "        'current_decay' : 0.0,\n",
        "        'voltage_decay' : 0.0,\n",
        "        'tau_grad'      : 0.03,\n",
        "        'scale_grad'    : 3,\n",
        "        'requires_grad' : True,\n",
        "      }\n",
        "    neuron_params_drop_1 = {**neuron_params_1, 'dropout' : slayer.neuron.Dropout(p=0.05),}\n",
        "\n",
        "    layers = []\n",
        "    for i in range(len(session_results['network']['ofs'])):\n",
        "      if i == len(session_results['network']['ofs']) - 1:\n",
        "        layer = slayer.block.cuba.Dense(neuron_params, session_results['network']['ofs'][i-1], session_results['network']['ofs'][i], weight_norm=True)\n",
        "      else:\n",
        "        input_size = session_results['network']['ofs'][i-1] if i > 0 else network_if\n",
        "        layer = slayer.block.cuba.Dense(neuron_params_drop, input_size, session_results['network']['ofs'][i], weight_norm=True, delay=isinstance(session_parameters['network']['max_delay'][i], int) and session_parameters['network']['max_delay'][i] > 0)\n",
        "      layers.append(layer)\n",
        "    self.blocks = torch.nn.ModuleList(layers)\n",
        "\n",
        "  def forward(self, spike):\n",
        "    \"\"\"\n",
        "    Defines the forward pass of the neural network.\n",
        "\n",
        "    Args:\n",
        "    - spike: Input spike data for the network.\n",
        "\n",
        "    Returns:\n",
        "    - Output spike after passing through the network layers.\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(len(session_results['network']['ofs'])):\n",
        "      spike = self.blocks[i](spike)\n",
        "    return spike\n",
        "\n",
        "  def grad_flow(self, path):\n",
        "    \"\"\"\n",
        "    Monitors and plots the gradient flow through the network layers.\n",
        "\n",
        "    Args:\n",
        "    - path: File path where the gradient flow plot will be saved.\n",
        "\n",
        "    Returns:\n",
        "    - A list containing gradient norms for each synapse in the network layers.\n",
        "    \"\"\"\n",
        "\n",
        "    grad = [b.synapse.grad_norm for b in self.blocks if hasattr(b, 'synapse')]\n",
        "\n",
        "    plt.figure()\n",
        "    plt.semilogy(grad)\n",
        "    plt.savefig(path + 'gradFlow.png')\n",
        "    plt.close()\n",
        "\n",
        "    return grad\n",
        "\n",
        "  def export_hdf5(self, filename):\n",
        "    \"\"\"\n",
        "    Exports the network configuration and parameters to an HDF5 file.\n",
        "\n",
        "    Args:\n",
        "    - filename: Path of the HDF5 file where the network configuration will be saved.\n",
        "    \"\"\"\n",
        "\n",
        "    h = h5py.File(filename, 'w')\n",
        "    layer = h.create_group('layer')\n",
        "    for i, b in enumerate(self.blocks):\n",
        "      b.export_hdf5(layer.create_group(f'{i}'))\n",
        "\n",
        "\n",
        "\n",
        "network_if = x_train.shape[1]\n",
        "\n",
        "if session_loaded and session_trained:\n",
        "  if session_results['training']['early_stopped']:\n",
        "    try:\n",
        "      model = torch.load(os.path.join(session_info['session_output_path'], session_folder_name, 'model_best.pt'), map_location=torch.device(session_results['device']))\n",
        "      print(f'Model correctly loaded: {os.path.join(session_info[\"session_output_path\"], session_folder_name, \"model_best.pt\")}\\nThe training will not be performed as it has already been completed in the previous training')\n",
        "    except FileNotFoundError as e:\n",
        "      raise FileNotFoundError(f\"Model file not found in {session_info['session_output_path']}: {e}\")\n",
        "    except KeyError as e:\n",
        "      raise KeyError(f\"Missing key in session_results: {e}\")\n",
        "    except Exception as e:\n",
        "      raise Exception(f\"Error loading the model: {e}\")\n",
        "\n",
        "    training_enable = False\n",
        "  else:\n",
        "    try:\n",
        "      model = torch.load(os.path.join(session_info['session_output_path'], session_folder_name, 'model_last.pt'), map_location=torch.device(session_results['device']))\n",
        "      print(f'Model correctly loaded: {os.path.join(session_info[\"session_output_path\"], session_folder_name, \"model_last.pt\")}\\nThe training will be concluded')\n",
        "    except FileNotFoundError as e:\n",
        "      raise FileNotFoundError(f\"Model file not found in {session_info['session_output_path']}: {e}\")\n",
        "    except KeyError as e:\n",
        "      raise KeyError(f\"Missing key in session_results: {e}\")\n",
        "    except Exception as e:\n",
        "      raise Exception(f\"Error loading the model: {e}\")\n",
        "\n",
        "    training_enable = True\n",
        "\n",
        "else:\n",
        "  session_results['network']['ofs'] = session_parameters['network']['ofs']\n",
        "  if session_results['network']['ofs'][-1] == None:\n",
        "    session_results['network']['ofs'][-1] = len(classes)\n",
        "  save_json(session_results, os.path.join(session_info['session_output_path'], session_folder_name, 'session_results.json'))\n",
        "\n",
        "  if session_parameters['training']['pretrained_model']:\n",
        "    model = torch.load(session_parameters['training']['pretrained_model'], map_location=torch.device(session_results['device']))\n",
        "    print(f'Pretrained model correctly loaded: {session_parameters[\"training\"][\"pretrained_model\"]}\\nA new training will be performed')\n",
        "  else:\n",
        "    model = Network().to(session_results['device'])\n",
        "    print(f'Model correctly created\\nA new training will be performed')\n",
        "\n",
        "  for i, max_delay in enumerate(session_parameters['network']['max_delay']):\n",
        "    if max_delay:\n",
        "      model.blocks[i].delay.max_delay = max_delay\n",
        "\n",
        "  training_enable = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okCDeBKsbZN8"
      },
      "source": [
        "## Training\n",
        "\n",
        "Configurations for the training process are based on the following parameters:\n",
        "\n",
        "- `epoch_max`: Maximum number of epochs for training; 'None' for no set limit.\n",
        "- `auto_unassign`: Indicates if resources should be automatically unassigned after training.\n",
        "- `pretrained_model`: Path to a pre-trained model, if used.\n",
        "- `optimizer`: Type of optimizer, like 'adam', for training.\n",
        "- `best_loss`: Criteria for selecting the best model, based on loss or accuracy.\n",
        "- `batch_size`: Size of data batches during training.\n",
        "- `true_rate` and `false_rate`: Firing rates for positive and negative class outputs in training.\n",
        "- `learning_rate_first`: Initial learning rate for the optimizer.\n",
        "- `learning_rate_decay` and `learning_rate_decay_steps`: Parameters for adjusting the learning rate over time.\n",
        "- `patience`: Epochs to wait for improvement before altering the training approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TCafgFvhCBM"
      },
      "source": [
        "### Training settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j36-NREJbXAx"
      },
      "outputs": [],
      "source": [
        "if training_enable:\n",
        "  error = slayer.loss.SpikeRate(true_rate=session_parameters['training']['true_rate'], false_rate=session_parameters['training']['false_rate'], reduction='sum').to(session_results['device'])\n",
        "  stats = slayer.utils.LearningStats()\n",
        "\n",
        "  if session_loaded and session_trained:\n",
        "    if session_parameters['training']['optimizer'] in optimizer_functions:\n",
        "      optimizer = optimizer_functions[session_parameters['training']['optimizer']](session_results['training']['learning_rate'][-1])\n",
        "    else:\n",
        "      raise ValueError(f\"Optimizer '{session_parameters['training']['optimizer']}' not supported\")\n",
        "\n",
        "    assistant = slayer.utils.Assistant(model, error, optimizer, stats, classifier=slayer.classifier.Rate.predict)\n",
        "    stats.training.min_loss = session_results['training']['train_loss_min']\n",
        "    stats.testing.min_loss = session_results['training']['valid_loss_min']\n",
        "    stats.training.max_accuracy = session_results['training']['train_accuracy_max']\n",
        "    stats.testing.max_accuracy = session_results['training']['valid_accuracy_max']\n",
        "\n",
        "    learning_rate = session_results['training']['learning_rate'][-1]\n",
        "\n",
        "  else:\n",
        "    if session_parameters['training']['optimizer'] in optimizer_functions:\n",
        "      optimizer = optimizer_functions[session_parameters['training']['optimizer']](session_parameters['training']['learning_rate_first'])\n",
        "    else:\n",
        "      raise ValueError(f\"Optimizer '{session_parameters['training']['optimizer']}' not supported\")\n",
        "\n",
        "    assistant = slayer.utils.Assistant(model, error, optimizer, stats, classifier=slayer.classifier.Rate.predict)\n",
        "\n",
        "    session_results['training'] = {\n",
        "      'optimizer' : None,              # UNEDITABLE # Training optimizer.\n",
        "      'done': False,                   # UNEDITABLE # Flag to indicate if the training process is complete.\n",
        "      'early_stopped': False,          # UNEDITABLE # Indicates if training was stopped early due to lack of improvement.\n",
        "      'epoch_best': None,              # UNEDITABLE # Epoch number where the best performance was achieved.\n",
        "      'epoch_last': None,              # UNEDITABLE # Last epoch number before training concluded.\n",
        "      'epoch_no_imporve': None,        # UNEDITABLE # For how many epochs has the model not improved on the validation set.\n",
        "      'train_loss_min': None,          # UNEDITABLE # Best training loss achieved.\n",
        "      'valid_loss_min': None,          # UNEDITABLE # Best validation loss achieved.\n",
        "      'train_accuracy_max': None,      # UNEDITABLE # Best training accuracy achieved.\n",
        "      'valid_accuracy_max': None,      # UNEDITABLE # Best validation accuracy achieved.\n",
        "      'learning_rate_best': None,      # UNEDITABLE # Best learning rate achieved during training.\n",
        "      'train_loss_best': None,         # UNEDITABLE # Best training loss achieved in the model_best.pt.\n",
        "      'valid_loss_best': None,         # UNEDITABLE # Best validation loss achieved in the model_best.pt.\n",
        "      'train_accuracy_best': None,     # UNEDITABLE # Best training accuracy achieved in the model_best.pt.\n",
        "      'valid_accuracy_best': None,     # UNEDITABLE # Best validation accuracy achieved in the model_best.pt.\n",
        "      'learning_rate': [],             # UNEDITABLE # History of learning rates used throughout training.\n",
        "      'train_loss': [],                # UNEDITABLE # History of training losses per epoch.\n",
        "      'valid_loss': [],                # UNEDITABLE # History of validation losses per epoch.\n",
        "      'train_acc': [],                 # UNEDITABLE # History of training accuracies per epoch.\n",
        "      'valid_acc': []                  # UNEDITABLE # History of validation accuracies per epoch.\n",
        "    }\n",
        "\n",
        "    session_results['post_training'] = {\n",
        "      'test_acc': None,                # UNEDITABLE # Test accuracy achieved after training is complete.\n",
        "      'test_acc_fakequant': None       # UNEDITABLE # Test accuracy achieved with fake quantization after training.\n",
        "    }\n",
        "\n",
        "    # Configuring and saving session information\n",
        "    session_results['training']['optimizer'] = str(optimizer).replace(\"\\n\",\"\").replace(\"    \",\", \")\n",
        "    session_results['training']['epoch_no_imporve'] = 0\n",
        "\n",
        "    save_json(session_results, os.path.join(session_info['session_output_path'], session_folder_name, 'session_results.json'))\n",
        "\n",
        "    # Setting learning rate parameter\n",
        "    learning_rate = session_parameters['training']['learning_rate_first']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4Xv13xAhHop"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL1AgWg8d_CM"
      },
      "outputs": [],
      "source": [
        "if training_enable:\n",
        "  while True:\n",
        "    if session_info['training']['epoch_max'] and session_results['training']['epoch_last'] and (session_results['training']['epoch_last'] + 1) >= session_info['training']['epoch_max']:\n",
        "      break\n",
        "\n",
        "    # training loop\n",
        "    for i, (input, label) in enumerate(train_dataloader):\n",
        "      output = assistant.train(input, label)\n",
        "\n",
        "    # validation loop\n",
        "    for i, (input, label) in enumerate(valid_dataloader):\n",
        "      output = assistant.test(input, label)\n",
        "\n",
        "    if session_results['training']['epoch_last'] == None:\n",
        "      session_results['training']['epoch_last'] = 0\n",
        "    else:\n",
        "      session_results['training']['epoch_last'] += 1\n",
        "\n",
        "    # session checkpoint\n",
        "    torch.save(model, os.path.join(session_info['session_output_path'], session_folder_name, 'model_last.pt'))\n",
        "\n",
        "    session_results['training']['learning_rate'].append(learning_rate)\n",
        "    session_results['training']['train_loss'].append(stats.training.loss)\n",
        "    session_results['training']['valid_loss'].append(stats.testing.loss)\n",
        "    session_results['training']['train_acc'].append(stats.training.accuracy)\n",
        "    session_results['training']['valid_acc'].append(stats.testing.accuracy)\n",
        "    session_results['training']['train_loss_min'] = stats.training.min_loss\n",
        "    session_results['training']['valid_loss_min'] = stats.testing.min_loss\n",
        "    session_results['training']['train_accuracy_max'] = stats.training.max_accuracy\n",
        "    session_results['training']['valid_accuracy_max'] = stats.testing.max_accuracy\n",
        "\n",
        "    save_json(session_results, os.path.join(session_info['session_output_path'], session_folder_name, 'session_results.json'))\n",
        "\n",
        "    print(\n",
        "      f'\\rE {session_results[\"training\"][\"epoch_last\"]}.   '                                        # Epoch\n",
        "      f'TL: {safe_format(stats.training.loss)} ({safe_format(stats.training.min_loss)}), '          # Training loss\n",
        "      f'VL: {safe_format(stats.testing.loss)} ({safe_format(stats.testing.min_loss)}).   '          # Validation loss\n",
        "      f'TA: {safe_format(stats.training.accuracy)} ({safe_format(stats.training.max_accuracy)}), '  # Training accuracy\n",
        "      f'VA: {safe_format(stats.testing.accuracy)} ({safe_format(stats.testing.max_accuracy)}).   ', # Validation accuracy\n",
        "      end=''\n",
        "    )\n",
        "\n",
        "    # check best model\n",
        "    if (session_parameters['training']['best_loss'] and stats.testing.best_loss) or (not session_parameters['training']['best_loss'] and stats.testing.best_accuracy):\n",
        "      torch.save(model, os.path.join(session_info['session_output_path'], session_folder_name, 'model_best.pt'))\n",
        "\n",
        "      session_results['training']['learning_rate_best'] = learning_rate\n",
        "      session_results['training']['train_loss_best'] = stats.training.loss\n",
        "      session_results['training']['valid_loss_best'] = stats.testing.loss\n",
        "      session_results['training']['train_accuracy_best'] = stats.training.accuracy\n",
        "      session_results['training']['valid_accuracy_best'] = stats.testing.accuracy\n",
        "      session_results['training']['epoch_best'] = session_results['training']['epoch_last']\n",
        "\n",
        "      save_json(session_results, os.path.join(session_info['session_output_path'], session_folder_name, 'session_results.json'))\n",
        "\n",
        "      session_results['training']['epoch_no_imporve'] = 0\n",
        "\n",
        "    else:\n",
        "      session_results['training']['epoch_no_imporve'] += 1\n",
        "\n",
        "      if session_results['training']['epoch_no_imporve'] >= session_parameters['training']['patience']:\n",
        "        # if learning_rate >= (1 / round(1 / ((session_parameters['training']['learning_rate_decay'] ** session_parameters['training']['learning_rate_decay_steps']) * session_parameters['training']['learning_rate_first']))):\n",
        "        if learning_rate >= round((session_parameters['training']['learning_rate_decay'] ** session_parameters['training']['learning_rate_decay_steps']) * session_parameters['training']['learning_rate_first'], 16):\n",
        "          learning_rate *= session_parameters['training']['learning_rate_decay']\n",
        "\n",
        "          if session_info['training']['bestmodel_lrchange']:\n",
        "            try:\n",
        "              model = torch.load(os.path.join(session_info['session_output_path'], session_folder_name, 'model_best.pt'), map_location=torch.device(session_results['device']))\n",
        "            except:\n",
        "              pass\n",
        "\n",
        "          save_json(session_results, os.path.join(session_info['session_output_path'], session_folder_name, 'session_results.json'))\n",
        "          for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = learning_rate\n",
        "          session_results['training']['epoch_no_imporve'] = 0\n",
        "          print(f'Reducing learning rate:', learning_rate)\n",
        "        else:\n",
        "          session_results['training']['early_stopped'] = True\n",
        "          save_json(session_results, os.path.join(session_info['session_output_path'], session_folder_name, 'session_results.json'))\n",
        "          print(f'Early stopping triggered after {session_results[\"training\"][\"epoch_last\"]+1} epochs!')\n",
        "          break\n",
        "      else:\n",
        "        print(f'Patience: {session_results[\"training\"][\"epoch_no_imporve\"]}.', end='')\n",
        "\n",
        "    stats.update()\n",
        "\n",
        "  session_results['training']['done'] = True\n",
        "  save_json(session_results, os.path.join(session_info['session_output_path'], session_folder_name, 'session_results.json'))\n",
        "\n",
        "  if session_info['training']['auto_unassign']:\n",
        "    time.sleep(30)\n",
        "    runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "p4ORPcaAC3_R",
        "qb99QxP3S0w6",
        "4npyn6MWA7r-"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
