{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDMWntTTR6Br"
      },
      "source": [
        "# **sEMG-force-based Gesture Recognition with Spiking Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikGnRJG9SkW-"
      },
      "source": [
        "## Dependencies and functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn2H3DdoSrQP"
      },
      "source": [
        "### Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMaSJrTfSzqL"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  # Import google.colab to check if running in Colab\n",
        "  import google.colab\n",
        "  from google.colab import runtime\n",
        "  colab_env = True  # Set flag to True for Colab environment\n",
        "except:\n",
        "  # Set flag to False if not in Colab\n",
        "  colab_env = False\n",
        "\n",
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import hashlib\n",
        "import natsort\n",
        "from datetime import datetime\n",
        "from natsort import natsorted\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "# Set Pandas display options\n",
        "pd.set_option('display.max_columns', None)        # Show all columns\n",
        "pd.set_option('display.max_rows', None)           # Show all rows\n",
        "pd.set_option('display.width', None)              # Show full width of the DataFrame\n",
        "pd.set_option('display.max_colwidth', None)       # Show full content of each cell\n",
        "\n",
        "# Basic scientific computing libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Advanced scientific computing and statistics\n",
        "from scipy.signal import butter, filtfilt, resample\n",
        "\n",
        "# Machine learning and data processing\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# PyTorch for deep learning\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "if colab_env:\n",
        "  !pip install snntorch &> /dev/null\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "from snntorch import functional as SF\n",
        "from snntorch import utils\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive/')\n",
        "  gdrive_path = '/content/gdrive/MyDrive/'  # Define Google Drive path in Colab\n",
        "except:\n",
        "  pass\n",
        "\n",
        "if colab_env:\n",
        "  !pip install wfdb &> /dev/null\n",
        "import wfdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjMPrW-RSvRN"
      },
      "source": [
        "### Function definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG7W2X1KbIG8"
      },
      "outputs": [],
      "source": [
        "def get_adam_optimizer(learning_rate):\n",
        "  \"\"\"\n",
        "  Creates and returns an Adam optimizer for a globally defined model with a specified learning rate.\n",
        "\n",
        "  Args:\n",
        "  learning_rate (float): The learning rate to be used for the optimizer.\n",
        "\n",
        "  Globals:\n",
        "  model: The globally defined model for which the optimizer is to be created.\n",
        "\n",
        "  Returns:\n",
        "  An instance of torch.optim.Adam configured with the specified learning rate.\n",
        "  \"\"\"\n",
        "  return torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "def hash_dictionary(dictionary):\n",
        "  \"\"\"\n",
        "  Generates a truncated MD5 hash for a given dictionary.\n",
        "\n",
        "  This function serializes the dictionary into a JSON string, ensuring the keys are sorted,\n",
        "  and then computes an MD5 hash of this string. The hash is then truncated to 8 characters.\n",
        "\n",
        "  Parameters:\n",
        "  dictionary: The dictionary to hash. It should be capable of being serialized into JSON.\n",
        "\n",
        "  Returns:\n",
        "  str: An 8-character string representing the truncated MD5 hash of the dictionary.\n",
        "\n",
        "  Example:\n",
        "  >>> hash_dictionary({'key1': 'value1', 'key2': 'value2'})\n",
        "  'e201065d'\n",
        "  \"\"\"\n",
        "\n",
        "  dict_string = json.dumps(dictionary, sort_keys=True)\n",
        "  return hashlib.md5(dict_string.encode()).hexdigest()\n",
        "\n",
        "\n",
        "\n",
        "def find_session_byhash(path, hash):\n",
        "  \"\"\"\n",
        "  Finds the last folder of a specific session based on a provided hash.\n",
        "\n",
        "  This function searches through subdirectories in the specified path, looking for a file named 'session_hash.txt' in each directory. It compares the content of this file, which should be the session's hash, with the provided hash parameter. If a match is found, it returns only the name of the last folder in the path of the corresponding directory.\n",
        "\n",
        "  Parameters:\n",
        "  path (str): The base path to start searching for session directories.\n",
        "  hash (str): The hash of the session to be found.\n",
        "\n",
        "  Returns:\n",
        "  str or None: The name of the last folder in the path of the directory containing the matching session hash, if found; otherwise, None.\n",
        "  \"\"\"\n",
        "\n",
        "  # Iterate through all 'session_parameters.json' files in the specified path\n",
        "  for root, dirs, files in os.walk(path):\n",
        "    for file in files:\n",
        "      if file == 'session_hash.txt':\n",
        "        file_path = os.path.join(root, file)\n",
        "\n",
        "        with open(file_path, 'r') as file:\n",
        "          session_hash = file.read().strip()\n",
        "\n",
        "        if session_hash == hash:\n",
        "          return os.path.basename(root)\n",
        "\n",
        "  return None\n",
        "\n",
        "\n",
        "\n",
        "def save_json(data, file_path, savedatetime=True):\n",
        "  \"\"\"\n",
        "  Saves a given dictionary as a JSON file at the specified file path, adding a timestamp.\n",
        "\n",
        "  Args:\n",
        "  - data (dict): The dictionary to be saved as JSON. If the file_path is 'session_results.json',\n",
        "                  the current datetime is added to this dictionary under the key 'datetime'.\n",
        "  - file_path (str): The file path where the JSON file will be saved.\n",
        "\n",
        "  This function takes a dictionary and a file path as inputs, adds the current datetime to the\n",
        "  dictionary if the file_path is 'session_results.json', and writes the dictionary as a JSON\n",
        "  file at the given path. It handles any exceptions related to file operations and prints\n",
        "  a relevant message in case of an error.\n",
        "  \"\"\"\n",
        "\n",
        "  if savedatetime and 'session_results.json' in file_path:\n",
        "    data['datetime'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "  with open(file_path, 'w') as file:\n",
        "    json.dump(data, file, indent=4)\n",
        "\n",
        "\n",
        "\n",
        "def load_json(file_path):\n",
        "  \"\"\"\n",
        "  Loads data from a JSON file into a Python dictionary.\n",
        "\n",
        "  Args:\n",
        "  - file_path (str): The file path of the JSON file to be loaded.\n",
        "\n",
        "  Returns:\n",
        "  - dict: The data loaded from the JSON file.\n",
        "\n",
        "  This function takes a file path as input and reads the JSON file from the given path into a Python dictionary.\n",
        "  It handles any exceptions related to file operations and prints a relevant message in case of an error.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(file_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "def normalize_json_files(path, dict_name):\n",
        "  \"\"\"\n",
        "  Normalizza i file JSON in una directory specificata e compila i loro contenuti in un DataFrame Pandas.\n",
        "\n",
        "  Questa funzione cerca i file JSON con un nome specificato nel percorso di directory fornito. Normalizza ciascun file JSON in un DataFrame Pandas, estrae il nome dell'hash della sessione dalla cartella in cui ogni file è stato trovato, e compila tutti i dati in un unico DataFrame.\n",
        "\n",
        "  Parametri:\n",
        "  path (str): Il percorso del file dove si trovano i file JSON specificati.\n",
        "  dict_name (str): Il nome dei file JSON da cercare (es., 'session_parameters.json').\n",
        "\n",
        "  Restituisce:\n",
        "  pd.DataFrame: Un DataFrame contenente i dati normalizzati da ciascun file JSON, con la colonna 'session_hash' come prima colonna, indicante l'hash della sessione della cartella dove ogni file JSON è stato trovato.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create an empty DataFrame\n",
        "  aggregated_df = pd.DataFrame()\n",
        "\n",
        "  # Iterate through all 'session_parameters.json' files in the specified path\n",
        "  for root, dirs, files in os.walk(path):\n",
        "    for file in files:\n",
        "      if file == dict_name:\n",
        "        file_path = os.path.join(root, file)\n",
        "\n",
        "        # Read and normalize the JSON file\n",
        "        with open(file_path, 'r') as f:\n",
        "          data = json.load(f)\n",
        "        normalized_data = pd.json_normalize(data)\n",
        "\n",
        "        with open(os.path.join(root, 'session_hash.txt'), 'r') as file:\n",
        "          session_hash = file.read().strip()\n",
        "\n",
        "        # Insert 'FolderName' as the first column\n",
        "        normalized_data.insert(0, 'session_hash', session_hash)\n",
        "\n",
        "        # Add the normalized data to the aggregated DataFrame\n",
        "        aggregated_df = pd.concat([aggregated_df, normalized_data], ignore_index=True, sort=False)\n",
        "\n",
        "  return aggregated_df\n",
        "\n",
        "\n",
        "\n",
        "def get_sorted_dbfiles(directory):\n",
        "  \"\"\"\n",
        "  Generates a naturally sorted list of unique file names (without extensions)\n",
        "  in a specified directory, including its subdirectories.\n",
        "\n",
        "  Args:\n",
        "  directory (str): The path to the directory to be scanned.\n",
        "\n",
        "  Returns:\n",
        "  list: A list of sorted unique file names without extensions.\n",
        "  \"\"\"\n",
        "  # Using a set to avoid duplicates\n",
        "  file_names = set()\n",
        "\n",
        "  # Recursively scanning the directory and subdirectories\n",
        "  for root, dirs, files in os.walk(directory):\n",
        "    for file in files:\n",
        "      # Removing the file extension\n",
        "      file_name_without_extension = os.path.splitext(file)[0]\n",
        "      full_path_without_extension = os.path.join(root, file_name_without_extension)\n",
        "      file_names.add(full_path_without_extension)\n",
        "\n",
        "  # Naturally sorting the file names\n",
        "  return natsort.natsorted(file_names)\n",
        "\n",
        "\n",
        "\n",
        "def extract_file_info(string):\n",
        "  \"\"\"\n",
        "  Extracts information about subject, session, dataset_type, data_type, object number, and sample\n",
        "  from a given string using regular expressions, ignoring the file extension.\n",
        "  Also returns the original string and a modified version of the string with 'object' in place of the object field.\n",
        "\n",
        "  :param string: The input string to extract data from.\n",
        "  :return: A dictionary containing extracted information, the original string, and the modified string.\n",
        "  \"\"\"\n",
        "\n",
        "  # Defining the regular expression pattern\n",
        "  pattern = r\"subject(\\d+)_session(\\d+)/(\\w+)_([a-z]+)_\\w*(\\d+)_sample(\\d+)\"\n",
        "\n",
        "  # Searching for the pattern in the string\n",
        "  match = re.search(pattern, string)\n",
        "\n",
        "  if match:\n",
        "    # Create a modified string with 'object' in place of the object field\n",
        "    modified_string = re.sub(r'_([a-z]+)_', f'_datatype_', string)\n",
        "\n",
        "    return {\n",
        "      \"subject\": int(match.group(1)),\n",
        "      \"session\": int(match.group(2)),\n",
        "      \"dataset_type\": match.group(3),\n",
        "      \"data_type\": match.group(4),\n",
        "      \"object\": int(match.group(5)),\n",
        "      \"sample\": int(match.group(6)),\n",
        "      \"modified_string\": modified_string\n",
        "      }\n",
        "\n",
        "  else:\n",
        "    return \"No match found\"\n",
        "\n",
        "\n",
        "\n",
        "def signal_downsample(signal, orig_fs, target_fs):\n",
        "  \"\"\"\n",
        "  Downsamples a signal from an original sampling frequency to a target frequency using fractional resampling with anti-aliasing filtering.\n",
        "  This method avoids temporal distortion by accurately calculating the number of samples in the downsampled signal.\n",
        "\n",
        "  :param signal: The input signal to be downsampled, assumed to be a 2D numpy array where rows represent time and columns represent channels.\n",
        "  :param orig_fs: Original sampling frequency of the signal.\n",
        "  :param target_fs: Target sampling frequency.\n",
        "  :return: Downsampled signal.\n",
        "  \"\"\"\n",
        "\n",
        "  # Number of channels\n",
        "  num_channels = signal.shape[1]\n",
        "\n",
        "  # Duration of the signal in seconds\n",
        "  duration = signal.shape[0] / orig_fs\n",
        "\n",
        "  # Accurate calculation of the number of samples in the downsampled signal\n",
        "  num_samples = int(duration * target_fs)\n",
        "\n",
        "  # Design an anti-aliasing Butterworth filter\n",
        "  nyquist_target = target_fs / 2\n",
        "  b, a = butter(N=5, Wn=nyquist_target, btype='low', fs=orig_fs)\n",
        "\n",
        "  # Initialize the downsampled signal array\n",
        "  downsampled_signal = np.zeros((num_samples, num_channels))\n",
        "\n",
        "  # Process each channel\n",
        "  for i in range(num_channels):\n",
        "    # Apply the filter to the channel\n",
        "    filtered_channel = filtfilt(b, a, signal[:, i])\n",
        "\n",
        "    # Resample the filtered channel\n",
        "    downsampled_signal[:, i] = resample(filtered_channel, num_samples)\n",
        "\n",
        "  return downsampled_signal\n",
        "\n",
        "\n",
        "\n",
        "def signal_upsample(signal, orig_fs, target_fs):\n",
        "  \"\"\"\n",
        "  Upsamples a signal from an original sampling frequency to a target frequency using fractional resampling with a low-pass filter.\n",
        "  This method avoids temporal distortion by accurately calculating the number of samples in the upsampled signal.\n",
        "\n",
        "  :param signal: The input signal to be upsampled, assumed to be a 2D numpy array where rows represent time and columns represent channels.\n",
        "  :param orig_fs: Original sampling frequency of the signal.\n",
        "  :param target_fs: Target sampling frequency.\n",
        "  :return: Upsampled signal.\n",
        "  \"\"\"\n",
        "\n",
        "  # Number of channels\n",
        "  num_channels = signal.shape[1]\n",
        "\n",
        "  # Duration of the signal in seconds\n",
        "  duration = signal.shape[0] / orig_fs\n",
        "\n",
        "  # Accurate calculation of the number of samples in the upsampled signal\n",
        "  num_samples = int(duration * target_fs)\n",
        "\n",
        "  # Design a low-pass Butterworth filter\n",
        "  nyquist_orig = orig_fs / 2\n",
        "  b, a = butter(N=5, Wn=nyquist_orig, btype='low', fs=target_fs)\n",
        "\n",
        "  # Initialize the upsampled signal array\n",
        "  upsampled_signal = np.zeros((num_samples, num_channels))\n",
        "\n",
        "  # Process each channel\n",
        "  for i in range(num_channels):\n",
        "    # Resample the channel\n",
        "    channel_upsampled = resample(signal[:, i], num_samples)\n",
        "\n",
        "    # Apply the filter to the upsampled channel\n",
        "    upsampled_signal[:, i] = filtfilt(b, a, channel_upsampled)\n",
        "\n",
        "  return upsampled_signal\n",
        "\n",
        "\n",
        "\n",
        "def lowpass_filter(signal, cutoff_frequency, sampling_rate, order=5):\n",
        "  \"\"\"\n",
        "  Applies a low-pass Butterworth filter to each channel of a multi-channel signal.\n",
        "\n",
        "  Parameters:\n",
        "  - signal (numpy.ndarray): The input signal array, where each column represents a channel.\n",
        "  - cutoff_frequency (int or float, optional): The cutoff frequency of the low-pass filter in Hz.\n",
        "  - sampling_rate (int or float, optional): The sampling rate of the signal in Hz.\n",
        "  - order (int, optional): The order of the Butterworth filter. Default is 5.\n",
        "\n",
        "  Returns:\n",
        "  - numpy.ndarray: The filtered signal, with the same shape as the input signal.\n",
        "  \"\"\"\n",
        "\n",
        "  num_channels = signal.shape[1]\n",
        "  b, a = butter(N=order, Wn=cutoff_frequency, btype='low', fs=sampling_rate)\n",
        "\n",
        "  filtered_signal = np.zeros_like(signal)  # Initialize a signal array of zeros with the same shape as the input signal\n",
        "  for i in range(num_channels):\n",
        "    filtered_signal[:, i] = filtfilt(b, a, signal[:, i])\n",
        "\n",
        "  return filtered_signal\n",
        "\n",
        "\n",
        "\n",
        "def process_data(array, max_derivative_order, delta_value, zc_enable=False, zc_max_derivative_order = 0, zc_value = None): # , ch_start=None, ch_end=None\n",
        "  \"\"\"\n",
        "  Processes an array of data by calculating derivatives up to a specified maximum order and generating a spike array based on delta values. Optionally, it can also detect zero crossings with a specified tolerance and mark these crossings in the spike array.\n",
        "\n",
        "  Args:\n",
        "    array (np.ndarray): Numpy array containing the data to be processed. Expected to be a 2D array where rows represent observations and columns represent different variables.\n",
        "    max_derivative_order (int): Maximum order of derivative to calculate for each data column, not considering zero crossing detection.\n",
        "    delta_value (list or np.ndarray): List or array of delta values used for determining spikes. Length should match `max_derivative_order + 1`.\n",
        "    zc_enable (bool, optional): Enables zero crossing detection if set to True. Defaults to False.\n",
        "    zc_max_derivative_order (int, optional): Maximum derivative order to consider for zero crossing detection. Used only if `zc_enable` is True.\n",
        "    zc_value (list or np.ndarray, optional): List or array of tolerance values for zero crossing detection for each derivative order. Length should match `zc_max_derivative_order + 1`. Used only if `zc_enable` is True.\n",
        "\n",
        "  Returns:\n",
        "    tuple: A tuple containing two np.ndarray elements:\n",
        "      - The first array is the expanded array that includes calculated derivatives for each data column up to the specified `global_max_derivative_order`, which is the maximum of `max_derivative_order` and `zc_max_derivative_order` if zero crossing detection is enabled, otherwise it's just `max_derivative_order`.\n",
        "      - The second array is the spike array, where spikes (represented by 1s) indicate significant changes in the data or zero crossings, based on `delta_value` and `zc_value`.\n",
        "\n",
        "  Note:\n",
        "    The function assumes that the input `array` is already preprocessed and ready for derivative calculation and spike detection. The lengths of `delta_value` and `zc_value` must be appropriate for the specified derivative orders. The derivative and spike arrays are sized based on the `global_max_derivative_order` to accommodate the computation of derivatives and spike detection across all orders.\n",
        "  \"\"\"\n",
        "\n",
        "  old_dim_size = array.shape[1]\n",
        "  global_max_derivative_order = max(max_derivative_order, zc_max_derivative_order) if zc_enable else max_derivative_order\n",
        "\n",
        "  spike_array = np.zeros((array.shape[0] - global_max_derivative_order * 4, (old_dim_size * (max_derivative_order + 1) * 2) + (old_dim_size * (zc_max_derivative_order + 1) if zc_enable else 0)), dtype=np.int8)\n",
        "  derivative_array = np.zeros((array.shape[0], old_dim_size * (global_max_derivative_order + 1)))\n",
        "  derivative_array[:, :old_dim_size] = array\n",
        "\n",
        "  if global_max_derivative_order:\n",
        "    for n in range(1, global_max_derivative_order + 1):\n",
        "      for i in range(old_dim_size):\n",
        "        for j in range(array[:, i].shape[0] - n * 4):\n",
        "          derivative_array[j + n * 2, old_dim_size * n + i] = - derivative_array[j, old_dim_size * (n - 1) + i] - 2 * derivative_array[j + 1, old_dim_size * (n - 1) + i] + 2 * derivative_array[j + 2, old_dim_size * (n - 1) + i] + derivative_array[j + 3, old_dim_size * (n - 1) + i]\n",
        "\n",
        "    # Trim the arrays to the minimum shape\n",
        "    derivative_array = derivative_array[global_max_derivative_order * 2 : - global_max_derivative_order * 2, :]\n",
        "\n",
        "  for n in range(max_derivative_order + 1):\n",
        "    for i in range(old_dim_size):\n",
        "      dc_val = derivative_array[0, old_dim_size * n + i] # or = 0\n",
        "      for k, j in enumerate(derivative_array[:, old_dim_size * n + i]):\n",
        "        if j > dc_val + delta_value[n]:\n",
        "          dc_val = j\n",
        "          spike_array[k, (old_dim_size * n + i) * 2] = 1\n",
        "          spike_array[k, (old_dim_size * n + i) * 2 + 1] = 0\n",
        "        elif j < dc_val - delta_value[n]:\n",
        "          dc_val = j\n",
        "          spike_array[k, (old_dim_size * n + i) * 2] = 0\n",
        "          spike_array[k, (old_dim_size * n + i) * 2 + 1] = 1\n",
        "        else:\n",
        "          spike_array[k, (old_dim_size * n + i) * 2] = 0\n",
        "          spike_array[k, (old_dim_size * n + i) * 2 + 1] = 0\n",
        "\n",
        "  if zc_enable:\n",
        "    for n in range(zc_max_derivative_order + 1):\n",
        "      for i in range(old_dim_size):\n",
        "        zc_state = 'above' if derivative_array[0, old_dim_size * n + i] > zc_value[n] else 'below' if derivative_array[0, old_dim_size * n + i] < -zc_value[n] else 'inside'\n",
        "        for k, j in enumerate(derivative_array[:, old_dim_size * n + i]):\n",
        "          current_state = 'above' if j > zc_value[n] else 'below' if j < -zc_value[n] else zc_state\n",
        "          if current_state != zc_state and current_state != 'inside':\n",
        "            spike_array[k, (old_dim_size * (max_derivative_order + 1) * 2) + (old_dim_size * n + i)] = 1\n",
        "            zc_state = current_state\n",
        "\n",
        "  return derivative_array, spike_array\n",
        "\n",
        "def safe_format(value, fmt=\".5f\"):\n",
        "  \"\"\"\n",
        "  Safely format a given value, handling None cases.\n",
        "\n",
        "  Args:\n",
        "  value: The value to be formatted. Can be None or any value that supports formatting.\n",
        "  fmt: The format string (default is \".5f\").\n",
        "\n",
        "  Returns:\n",
        "  A formatted string according to 'fmt' if 'value' is not None, otherwise \"N/D\" (Not Available).\n",
        "  \"\"\"\n",
        "\n",
        "  return f\"{value:{fmt}}\" if value is not None else \"N/D\"\n",
        "\n",
        "def progress_bar(iteration, total, start_time, initial_progress=0, length=25, unit='', prefix='', suffix='', init=False):\n",
        "  \"\"\"\n",
        "  Displays a progress bar to monitor the progress of an iterative process.\n",
        "\n",
        "  Args:\n",
        "  iteration (int): The current iteration number.\n",
        "  total (int): The total number of iterations.\n",
        "  start_time (float): The start time of the process (typically obtained using time.time()).\n",
        "  initial_progress (int, optional): Initial progress for ETA calculation. Default is 0.\n",
        "  length (int, optional): The length of the progress bar. Default is 34.\n",
        "  unit (str, optional): The measurement unit for the iteration counter. Default is an empty string.\n",
        "  prefix (str, optional): A prefix to add before the progress bar. Default is an empty string.\n",
        "  suffix (str, optional): A suffix to add after the progress bar. Default is an empty string.\n",
        "  init (bool, optional): If True, displays only the progress bar without time information. Default is False.\n",
        "\n",
        "  Returns:\n",
        "  None: The function does not return anything but prints the progress bar with additional information.\n",
        "\n",
        "  The function updates the progress bar based on the current iteration and calculates the ETA (Estimated Time of Arrival)\n",
        "  based on elapsed time and progress. If 'init' is True, it only prints the progress bar without calculating the ETA.\n",
        "  \"\"\"\n",
        "\n",
        "  if iteration is None:\n",
        "    iteration = 0\n",
        "\n",
        "  unit = unit if not unit else ' ' + unit\n",
        "  prefix = prefix if not prefix else prefix + ' '\n",
        "  suffix = suffix if not suffix else ' ' + suffix\n",
        "\n",
        "  # Complete the bar on the last iteration\n",
        "  if iteration == total:\n",
        "    bar = '█' * length\n",
        "    percent = 100\n",
        "  else:\n",
        "    percent = 100 * (iteration / float(total))\n",
        "    filled_length = int(length * iteration // total)\n",
        "    full_char = '█'\n",
        "    partial_chars = [' ', '▏', '▎', '▍', '▌', '▋', '▊', '▉']\n",
        "    partial_index = int((length * iteration % total) / (total / len(partial_chars)))\n",
        "    bar = full_char * filled_length + partial_chars[partial_index] + ' ' * (length - filled_length - 1)\n",
        "\n",
        "  if init:\n",
        "    sys.stdout.write(f'\\r{prefix}|{bar}| {iteration}/{total}{unit}{suffix}')\n",
        "  else:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    eta = (elapsed_time / max(1, iteration - initial_progress)) * (total - iteration)\n",
        "    sec_per_iter = elapsed_time / max(1, iteration - initial_progress)\n",
        "\n",
        "    elapsed_time_str = time.strftime('%H:%M:%S', time.gmtime(elapsed_time))\n",
        "    eta_str = time.strftime('%H:%M:%S', time.gmtime(eta))\n",
        "\n",
        "    sys.stdout.write(f'\\r{prefix}|{bar}| {iteration}/{total}{unit} [{percent:.1f}%, {elapsed_time_str}<{eta_str}, {sec_per_iter:.2f}s/it]{suffix}{\" \"*20}')\n",
        "\n",
        "  sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_mvc_wfdb(path, subject, session, finger, direction):\n",
        "  record_name = f\"{path}/mvc_dataset/subject{subject:02d}_session{session}/mvc_force_finger{finger}_{direction}\"\n",
        "  try:\n",
        "    record = wfdb.rdrecord(record_name)\n",
        "    data = record.p_signal\n",
        "  except FileNotFoundError:\n",
        "    raise FileNotFoundError(f\"Failed to open: {record_name}\")\n",
        "\n",
        "  return data\n",
        "\n",
        "def get_mvc_finger(path, subject, session, finger):\n",
        "  mvc = []\n",
        "  for i in ['flexion', 'extension']:\n",
        "    force_data = load_mvc_wfdb(path, subject, session, finger, i)[:, finger - 1]\n",
        "    force_data = np.abs(force_data)\n",
        "    force_data = np.sort(force_data)\n",
        "    mvc.append(np.mean(force_data[-200:]))\n",
        "  return mvc\n",
        "\n",
        "def get_mvc(path, subject, session):\n",
        "  mvc = []\n",
        "  for i in range(1, 6):\n",
        "    mvc.append(get_mvc_finger(path, subject, session, i))\n",
        "  return mvc\n",
        "\n",
        "def mvc_norm(signal, mvc_values):\n",
        "  result = np.empty_like(signal)\n",
        "  \n",
        "  for i in range(5):\n",
        "    NEG = mvc_values[i][0] # 'flexion'\n",
        "    POS = mvc_values[i][1] # 'extension'\n",
        "    # result[:, i][signal[:, i] > 0] = signal[:, i][signal[:, i] > 0] / POS\n",
        "    # result[:, i][signal[:, i] < 0] = signal[:, i][signal[:, i] < 0] / NEG\n",
        "    result[:, i] = np.where(signal[:, i] > 0, signal[:, i] / POS, signal[:, i])\n",
        "    result[:, i] = np.where(signal[:, i] < 0, signal[:, i] / NEG, result[:, i])\n",
        "  \n",
        "  return result\n",
        "\n",
        "def mvc_norm_set(data, mvc_path, mvc_session):\n",
        "    new_data = np.empty_like(data)\n",
        "    prev_subject = 0\n",
        "\n",
        "    for i, entry in enumerate(data):\n",
        "        subject = int(i / 5) + 1\n",
        "        if prev_subject != subject:\n",
        "            force_finger_mvc = get_mvc(mvc_path, subject, mvc_session)\n",
        "            prev_subject = subject\n",
        "        new_data[i] = mvc_norm(entry, force_finger_mvc)\n",
        "    \n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULqSulCP3IiN"
      },
      "source": [
        "### Class definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3UlNTV23L-p"
      },
      "outputs": [],
      "source": [
        "class HDsEMG_Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, target, spike_in):\n",
        "    self.num_samples = target.shape[0]\n",
        "    timesteps = target.shape[2]\n",
        "\n",
        "    target   = target.astype(np.float32)\n",
        "    spike_in = spike_in.astype(np.float32)\n",
        "\n",
        "    labels_target = []\n",
        "    lista = []\n",
        "    for i in range(self.num_samples):\n",
        "      target_i = target[i, :, :]\n",
        "      target_i = np.transpose(target_i, (1, 0))\n",
        "      labels_target.append(torch.from_numpy(target_i))\n",
        "\n",
        "      spike_ing = spike_in[i, :, :]\n",
        "      spike_ing = np.transpose(spike_ing, (1, 0))\n",
        "      lista.append(torch.from_numpy(spike_ing))\n",
        "\n",
        "    self.labels   = torch.stack(labels_target, dim=1)\n",
        "    self.features = torch.stack(lista, dim=1)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"Number of samples.\"\"\"\n",
        "    return self.num_samples\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"General implementation, but we only have one sample.\"\"\"\n",
        "    return self.features[:, idx, :], self.labels[:, idx, :]\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, timesteps, input = 1):\n",
        "    \"\"\"\n",
        "    Initializes the spiking neural network.\n",
        "\n",
        "    Parameters:\n",
        "    timesteps (int): Number of time steps for simulating the network.\n",
        "    input (int, optional): Number of input features. Default is 1.\n",
        "\n",
        "    The network consists of several layers, each followed by a Leaky integrate-and-fire (LIF) neuron model.\n",
        "    The layers `fc_0`, `fc_1`, `fc_2`, and `fc_3` are fully connected.\n",
        "    The neurons `lif_0`, `lif_1`, `lif_2`, and `li_3` are LIF models with parameters set according to `session_parameters`.\n",
        "\n",
        "    The network supports either a single threshold and beta decay value per layer (`threshold_perlayer` and `beta_decay_perlayer` set to True), or unique values for each neuron within a layer (set to False).\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.timesteps = timesteps # number of time steps to simulate the network\n",
        "    spike_grad = surrogate.fast_sigmoid() # surrogate gradient function\n",
        "\n",
        "    if session_parameters['network']['threshold_perlayer']:\n",
        "      thr_0 = session_parameters['network']['threshold'][0]\n",
        "      thr_1 = session_parameters['network']['threshold'][1]\n",
        "      thr_2 = session_parameters['network']['threshold'][2]\n",
        "      thr_3 = session_parameters['network']['threshold'][3]\n",
        "    else:\n",
        "      thr_0 = torch.ones(session_results['network']['ofs'][0]) * session_parameters['network']['threshold'][0]\n",
        "      thr_1 = torch.ones(session_results['network']['ofs'][1]) * session_parameters['network']['threshold'][1]\n",
        "      thr_2 = torch.ones(session_results['network']['ofs'][2]) * session_parameters['network']['threshold'][2]\n",
        "      thr_3 = torch.ones(session_results['network']['ofs'][3]) * session_parameters['network']['threshold'][3]\n",
        "\n",
        "    if session_parameters['network']['beta_decay_perlayer']:\n",
        "      beta_0 = session_parameters['network']['beta_decay'][0]\n",
        "      beta_1 = session_parameters['network']['beta_decay'][1]\n",
        "      beta_2 = session_parameters['network']['beta_decay'][2]\n",
        "      beta_3 = session_parameters['network']['beta_decay'][3]\n",
        "    else:\n",
        "      beta_0 = torch.ones(session_results['network']['ofs'][0]) * session_parameters['network']['beta_decay'][0]\n",
        "      beta_1 = torch.ones(session_results['network']['ofs'][1]) * session_parameters['network']['beta_decay'][1]\n",
        "      beta_2 = torch.ones(session_results['network']['ofs'][2]) * session_parameters['network']['beta_decay'][2]\n",
        "      if session_parameters['network']['beta_decay_outsingle']:\n",
        "        beta_3 = session_parameters['network']['beta_decay'][3]\n",
        "      else:\n",
        "        beta_3 = torch.ones(session_results['network']['ofs'][3]) * session_parameters['network']['beta_decay'][3]\n",
        "\n",
        "    self.fc_0 = torch.nn.Linear(in_features=input, out_features=session_results['network']['ofs'][0], bias = session_parameters['network']['bias'])\n",
        "    self.lif_0 = snn.Leaky(beta=beta_0, threshold=thr_0, learn_beta=session_parameters['network']['beta_decay_train'], spike_grad=spike_grad, learn_threshold=session_parameters['network']['threshold_train'], reset_mechanism=session_parameters['network']['reset'])\n",
        "\n",
        "    self.fc_1 = torch.nn.Linear(in_features=session_results['network']['ofs'][0], out_features=session_results['network']['ofs'][1], bias = session_parameters['network']['bias'])\n",
        "    self.lif_1 = snn.Leaky(beta=beta_1, threshold=thr_1, learn_beta=session_parameters['network']['beta_decay_train'], spike_grad=spike_grad, learn_threshold=session_parameters['network']['threshold_train'], reset_mechanism=session_parameters['network']['reset'])\n",
        "\n",
        "    self.fc_2 = torch.nn.Linear(in_features=session_results['network']['ofs'][1], out_features=session_results['network']['ofs'][2], bias = session_parameters['network']['bias'])\n",
        "    self.lif_2 = snn.Leaky(beta=beta_2, threshold=thr_2, learn_beta=session_parameters['network']['beta_decay_train'], spike_grad=spike_grad, learn_threshold=session_parameters['network']['threshold_train'], reset_mechanism=session_parameters['network']['reset'])\n",
        "\n",
        "    self.fc_3 = torch.nn.Linear(in_features=session_results['network']['ofs'][2], out_features=session_results['network']['ofs'][3], bias = session_parameters['network']['bias'])\n",
        "    self.li_3 = snn.Leaky(beta=beta_3, threshold=1, learn_beta=session_parameters['network']['beta_decay_train'], spike_grad=spike_grad, reset_mechanism='none')\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward pass for processing input data over multiple time steps.\n",
        "\n",
        "    Parameters:\n",
        "    x (Tensor): Input tensor with shape corresponding to [timesteps, batch_size, features].\n",
        "\n",
        "    Returns:\n",
        "    Tensor: Output tensor after processing through the network layers and time steps.\n",
        "\n",
        "    The method processes the input `x` through the network layers across the specified number of time steps.\n",
        "    For each time step, it computes the current (`cur_`) and spike (`spk_`) for each layer and updates the membrane potentials (`mem_`).\n",
        "    The output for the final layer (`mem_3`) is recorded at each time step and stacked to form the return tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initalize membrane potential\n",
        "    mem_0 = self.lif_0.init_leaky()\n",
        "    mem_1 = self.lif_1.init_leaky()\n",
        "    mem_2 = self.lif_2.init_leaky()\n",
        "    mem_3 = self.li_3.init_leaky()\n",
        "\n",
        "    # Empty lists to record outputs\n",
        "    mem_3_rec = []\n",
        "\n",
        "    # Loop over\n",
        "    for step in range(self.timesteps):\n",
        "      x_timestep = x[step, :, :]\n",
        "\n",
        "      cur_0 = self.fc_0(x_timestep)\n",
        "      spk_0, mem_0 = self.lif_0(cur_0, mem_0)\n",
        "\n",
        "      cur_1 = self.fc_1(spk_0)\n",
        "      spk_1, mem_1 = self.lif_1(cur_1, mem_1)\n",
        "\n",
        "      cur_2 = self.fc_2(spk_1)\n",
        "      spk_2, mem_2 = self.lif_2(cur_2, mem_2)\n",
        "\n",
        "      cur_3 = self.fc_3(spk_2)\n",
        "      _, mem_3 = self.li_3(cur_3, mem_3)\n",
        "\n",
        "      mem_3_rec.append(mem_3)\n",
        "\n",
        "    return torch.stack(mem_3_rec)\n",
        "\n",
        "class Net_(torch.nn.Module):\n",
        "  \"\"\"Simple spiking neural network in snntorch.\"\"\"\n",
        "\n",
        "  def __init__(self, timesteps, input = 1, target_variables = 1):\n",
        "    super().__init__()\n",
        "\n",
        "    l0 = train_setting['hyperparams'][0]\n",
        "    l1 = train_setting['hyperparams'][1]\n",
        "    l2 = train_setting['hyperparams'][2]\n",
        "\n",
        "    self.timesteps = timesteps # number of time steps to simulate the network\n",
        "    spike_grad = surrogate.fast_sigmoid() # surrogate gradient function\n",
        "\n",
        "    if train_setting['1thr4layer']:\n",
        "      thr_in       = train_setting['thr_init']\n",
        "      thr_hidden   = train_setting['thr_init']\n",
        "      thr_hidden2  = train_setting['thr_init']\n",
        "    else:\n",
        "      thr_in      = torch.ones(l0)*train_setting['thr_init']\n",
        "      thr_hidden  = torch.ones(l1)*train_setting['thr_init']\n",
        "      thr_hidden2 = torch.ones(l2)*train_setting['thr_init']\n",
        "\n",
        "    if train_setting['1beta4layer']:\n",
        "      beta_in      = train_setting['bet_init']\n",
        "      beta_hidden  = train_setting['bet_init']\n",
        "      beta_hidden2 = train_setting['bet_init']\n",
        "      beta_out     = train_setting['bet_init']\n",
        "    else:\n",
        "      beta_in      = torch.ones(l0)*train_setting['bet_init']\n",
        "      beta_hidden  = torch.ones(l1)*train_setting['bet_init']\n",
        "      beta_hidden2 = torch.ones(l2)*train_setting['bet_init']\n",
        "      beta_out     = torch.ones(target_variables)*train_setting['bet_init']\n",
        "      if train_setting['beta_out_single']:\n",
        "        beta_out   = train_setting['bet_init']\n",
        "\n",
        "    print(\"in_features, out_features\", input, l0)\n",
        "\n",
        "    # layer 1\n",
        "    self.fc_in = torch.nn.Linear(in_features=input, out_features=l0, bias = train_setting['bias'])\n",
        "    self.lif_in = snn.Leaky(beta=beta_in, threshold=thr_in, learn_beta=train_setting['decay_train'], spike_grad=spike_grad, learn_threshold=train_setting['thr_train'], reset_mechanism=train_setting['reset'])\n",
        "\n",
        "    # layer 2\n",
        "    self.fc_hidden = torch.nn.Linear(in_features=l0, out_features=l1, bias = train_setting['bias'])\n",
        "    self.lif_hidden = snn.Leaky(beta=beta_hidden, threshold=thr_hidden, learn_beta=train_setting['decay_train'], spike_grad=spike_grad, learn_threshold=train_setting['thr_train'], reset_mechanism=train_setting['reset'])\n",
        "\n",
        "    # layer 3\n",
        "    self.fc_hidden2 = torch.nn.Linear(in_features=l1, out_features=l2, bias = train_setting['bias'])\n",
        "    self.lif_hidden2 = snn.Leaky(beta=beta_hidden2, threshold=thr_hidden2, learn_beta=train_setting['decay_train'], spike_grad=spike_grad, learn_threshold=train_setting['thr_train'], reset_mechanism=train_setting['reset'])\n",
        "\n",
        "    # layer 4: leaky integrator neuron. Note the reset mechanism is disabled and we will disregard output spikes.\n",
        "    self.fc_out = torch.nn.Linear(in_features=l2, out_features = target_variables, bias = train_setting['bias'])\n",
        "    self.li_out = snn.Leaky(beta=beta_out, threshold=1.0, learn_beta=train_setting['decay_train'], spike_grad=spike_grad, reset_mechanism=\"none\")\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Forward pass for several time steps.\"\"\"\n",
        "\n",
        "    # Initalize membrane potential\n",
        "    mem_1 = self.lif_in.init_leaky()\n",
        "    mem_2 = self.lif_hidden.init_leaky()\n",
        "    mem_22 = self.lif_hidden2.init_leaky()\n",
        "    mem_3 = self.li_out.init_leaky()\n",
        "\n",
        "    # Empty lists to record outputs\n",
        "    mem_3_rec = []\n",
        "\n",
        "    # Loop over\n",
        "    for step in range(self.timesteps):\n",
        "        x_timestep = x[step, :, :]\n",
        "\n",
        "        cur_in = self.fc_in(x_timestep)\n",
        "        spk_in, mem_1 = self.lif_in(cur_in, mem_1)\n",
        "\n",
        "        cur_hidden = self.fc_hidden(spk_in)\n",
        "        spk_hidden, mem_2 = self.lif_hidden(cur_hidden, mem_2)\n",
        "\n",
        "        cur_hidden2 = self.fc_hidden2(spk_hidden)\n",
        "        spk_hidden2, mem_22 = self.lif_hidden2(cur_hidden2, mem_22)\n",
        "\n",
        "        cur_out = self.fc_out(spk_hidden2)\n",
        "        _, mem_3 = self.li_out(cur_out, mem_3)\n",
        "\n",
        "        mem_3_rec.append(mem_3)\n",
        "\n",
        "    return torch.stack(mem_3_rec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKcAybrTR5_F"
      },
      "source": [
        "## Load or create session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkB89NwYq7e9"
      },
      "outputs": [],
      "source": [
        "session_info = {\n",
        "  'project_name' : 'semg-force-gesturerecognition-snn', # Project name.\n",
        "  'session_loadbyhash' : '',                      # Complete this field to load a session using its hash.\n",
        "  'session_note' : '',                            # Session note.  lrmod3_20\n",
        "  'session_output_path' : 'output/',              # Set the path to the project's output directory\n",
        "\n",
        "  'overwrite' : False,                            # Flag to indicate if existing files should be overwritten.\n",
        "  'github_support' : True,                        # Indicates if GitHub support is enabled.\n",
        "  'repository_branch' : None,                     # Branch of the repository to use, default is 'main'.\n",
        "  'lazygit_support' : False,                      # Indicates if LazyGit support is enabled.\n",
        "\n",
        "  'dataset' : {\n",
        "    'dataset_path' : '../datasets/emg/',          # Path to the dataset directory.\n",
        "    'plot_exampledata' : True                     # Flag to indicate if example data from the dataset should be plotted.\n",
        "  },\n",
        "\n",
        "  'training' : {\n",
        "    'epoch_max' : 800,                            # Maximum number of training epochs. If set to None, there will be no precise number of maximum epochs.\n",
        "    'bestmodel_lrchange' : True,                  # If set to True, the model will revert to the best model state when a learning rate change occurs.\n",
        "    'auto_unassign' : False,                      # Flag to indicate automatic unassignment of resources post-training.\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SegiEXtmEyWM"
      },
      "source": [
        "### Existing session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH1ABsr245ma"
      },
      "source": [
        "#### Existing session info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AevK1FOP4vPq"
      },
      "outputs": [],
      "source": [
        "# Normalize the JSON files and create a DataFrame\n",
        "df = normalize_json_files(session_info['session_output_path'], 'session_info.json')\n",
        "\n",
        "# List of columns to ignore\n",
        "columns_to_ignore = []\n",
        "\n",
        "# Select all columns except those to ignore\n",
        "columns_to_keep = [col for col in df.columns if col not in columns_to_ignore]\n",
        "df_filtered = df[columns_to_keep]\n",
        "\n",
        "# Display the filtered DataFrame\n",
        "pd.DataFrame(df_filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ygwpVf64_Pl"
      },
      "source": [
        "#### Existing session paramters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JxWETdXanDU"
      },
      "outputs": [],
      "source": [
        "# Normalize the JSON files and create a DataFrame\n",
        "df = normalize_json_files(session_info['session_output_path'], 'session_parameters.json')\n",
        "\n",
        "# List of columns to ignore\n",
        "columns_to_ignore = []\n",
        "\n",
        "# Select all columns except those to ignore\n",
        "columns_to_keep = [col for col in df.columns if col not in columns_to_ignore]\n",
        "df_filtered = df[columns_to_keep]\n",
        "\n",
        "# Display the filtered DataFrame\n",
        "pd.DataFrame(df_filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7yMuwnu5JpP"
      },
      "source": [
        "#### Existing session results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnGLCutlapVm"
      },
      "outputs": [],
      "source": [
        "# Normalize the JSON files and create a DataFrame\n",
        "df = normalize_json_files(session_info['session_output_path'], 'session_results.json')\n",
        "\n",
        "# List of columns to ignore\n",
        "columns_to_ignore = ['hash_name', 'colab_env', 'device', 'training.optimizer', 'training.train_loss', 'training.valid_loss', 'training.learning_rate']\n",
        "\n",
        "# Select all columns except those to ignore\n",
        "columns_to_keep = [col for col in df.columns if col not in columns_to_ignore]\n",
        "df_filtered = df[columns_to_keep]\n",
        "\n",
        "# Display the filtered DataFrame\n",
        "pd.DataFrame(df_filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFSwbJg-E5Ks"
      },
      "source": [
        "### Session parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm2xRGfFOt6Z"
      },
      "outputs": [],
      "source": [
        "session_parameters = {\n",
        "  'hash_seed' : None,                             # Hash seed. Default None.\n",
        "  'dataset' : {\n",
        "    'dataset_subpath' : 'hd-semg/2.0.0/1dof_dataset/', # Subpath to the dataset directory. Currently tested {'ninapro/db1/', 'ninapro/db2/', 'ninapro/db5/'}.\n",
        "    'dataset_format' : 'wfdb',                    # Dataset format. Currently supported {'wfdb', 'numpy'}.\n",
        "    'in_data_freq_hz' : 2048,                     # Frequency of input data sampling in Hertz. None if the sampling frequency is not changed.\n",
        "    'out_data_freq_hz' : 100,                     # Frequency of output data sampling in Hertz. None if is equal to input data sampling in Hertz.\n",
        "    'out_data_filterfreq_hz' : 5,                 # ... None if is equal to output data sampling in Hertz.\n",
        "    'new_in_data_freq_hz' : 1000,                 # Frequency of input data sampling in Hertz. None if the sampling frequency is not changed.\n",
        "    'new_out_data_freq_hz' : 1000,                # Frequency of output data sampling in Hertz. None if is equal to output data sampling in Hertz.\n",
        "    'subjects' : [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],                              # List of subject IDs (enumarate) to include in the dataset. Set an empty list to include all subjects.\n",
        "    'sessions' : [2],\n",
        "    'objects_file' : [],\n",
        "    'objects_class' : [],\n",
        "    'samples' : [],\n",
        "    'channel_start' : None,                       # Starting index of channels to consider (None for the first possible channel).\n",
        "    'channel_end' : None,                         # Ending index of channels to consider (None for the last possible channel).\n",
        "    'channels_downsampling' : 2,\n",
        "    'max_derivative_order' : 0,                   # Maximum order of derivative to calculate for the data.\n",
        "    'delta_value' : 0.0165*2,                     # Value of delta for delta encoding. Set a list of values if you want different values per layer. Tested: 0.0165 at 100 Hz\n",
        "    'zc_enable' : False,\n",
        "    'zc_max_derivative_order' : 0,\n",
        "    'zc_value' : None,                            # [0.0175, 0.06]\n",
        "    'force_gain' : 1000,\n",
        "    'mvc_norm' : True, \n",
        "\n",
        "    'train_samples' : [2, 3],                     # Object to be used for training. 3... [2, 3]\n",
        "    'valid_samples' : [1],                        # Object to be used for validation.\n",
        "    'test_samples' : [1],                         # Object to be used for testing.\n",
        "\n",
        "    'random_seed' : 0,                            # Seed for random number generation to ensure reproducibility.\n",
        "  },\n",
        "\n",
        "  'network' : {\n",
        "    'ofs' : [64, 128, 64, None],                  # Output feature values for network layers, the last feature output value (number of output classes) is calculated automatically if set to None.\n",
        "    'bias' : False,                               # True if a bias is inserted in each neuron\n",
        "    'threshold' : 0.1,                            # Threshold value. Set a list of values if you want different values per layer.\n",
        "    'threshold_train' : False,                    # True if the threshold has been trained\n",
        "    'threshold_perlayer' : True,                  # True: a single threshold value per layer; False: a unique threshold value for each neuron\n",
        "    'beta_decay' : 0.9,                           # Beta decay value. Set a list of values if you want different values per layer.\n",
        "    'beta_decay_train' : True,                    # True if the beta decay has been trained\n",
        "    'beta_decay_perlayer' : False,                # True: a single beta decay value per layer; False: a unique beta decay value for each neuron\n",
        "    'beta_decay_outsingle' : False,               # True: a single decay value for neurons in the last layer\n",
        "    'reset' : 'subtract',                         # None; subtract; zero; mechanism for neuron reset\n",
        "  },\n",
        "\n",
        "  'training' : {\n",
        "    'pretrained_model' : None,                    # Set a pretrained-folder path if needed.\n",
        "    # 'optimizer' : 'adam',                       # Optimizer to use for training, currently supported {'adam'}.\n",
        "    'best_loss' : True,                           # If set to True, the best model will be selected during training based on the lowest loss on the training set. If set to False, the selection will be based on the lowest loss on the validation set.\n",
        "    'batch_size' : 32,                            # Batch size for training.\n",
        "    'learning_rate_first' : 0.001,                # Initial learning rate for the optimizer.\n",
        "    'learning_rate_decay' : 1/3,                  # Factor by which the learning rate decays.\n",
        "    'learning_rate_decay_steps' : 4,              # Number of times the learning_rate_decay is applied.\n",
        "    'patience' : 30                               # Number of epochs to wait for improvement before stopping training or changing the learning rate.\n",
        "  }\n",
        "}\n",
        "\n",
        "session_results = {\n",
        "  'hash_name' : None,                # UNEDITABLE # Hash of the session_parameters dictionary.\n",
        "  'colab_env' : None,                # UNEDITABLE # Google Colab support.\n",
        "  'device' : None,                   # UNEDITABLE # Hardware support.\n",
        "  'datetime' : None,                 # UNEDITABLE # Last modification.\n",
        "\n",
        "  'network': {\n",
        "    'ofs': None                      # UNEDITABLE # Final offset values determined for the network layers.\n",
        "  }\n",
        "}\n",
        "\n",
        "# Dictionary serving as a repository for various optimizer functions.\n",
        "optimizer_functions = {\n",
        "  'adam': get_adam_optimizer\n",
        "}\n",
        "\n",
        "if not session_parameters['dataset']['in_data_freq_hz'] or not session_parameters['dataset']['new_in_data_freq_hz'] or session_parameters['dataset']['in_data_freq_hz'] == session_parameters['dataset']['new_in_data_freq_hz']:\n",
        "  session_parameters['dataset']['in_data_freq_hz'] = None\n",
        "  session_parameters['dataset']['new_in_data_freq_hz'] = None\n",
        "\n",
        "if not session_parameters['dataset']['out_data_freq_hz'] or (not session_parameters['dataset']['new_out_data_freq_hz'] and not session_parameters['dataset']['out_data_filterfreq_hz']) or (not session_parameters['dataset']['out_data_filterfreq_hz'] and session_parameters['dataset']['out_data_freq_hz'] == session_parameters['dataset']['new_out_data_freq_hz']):\n",
        "  session_parameters['dataset']['out_data_freq_hz'] = None\n",
        "  session_parameters['dataset']['out_data_filterfreq_hz'] = None\n",
        "  session_parameters['dataset']['new_out_data_freq_hz'] = None\n",
        "\n",
        "if not session_parameters['dataset']['out_data_freq_hz'] and (session_parameters['dataset']['out_data_filterfreq_hz'] or session_parameters['dataset']['new_out_data_freq_hz']):\n",
        "  print('\"out_data_freq_hz\" value is missing')\n",
        "  raise\n",
        "\n",
        "if not isinstance(session_parameters['dataset']['delta_value'], list):\n",
        "  session_parameters['dataset']['delta_value'] = [session_parameters['dataset']['delta_value']] * (session_parameters['dataset']['max_derivative_order'] + 1) # len(session_parameters['network']['ofs'])\n",
        "\n",
        "if not isinstance(session_parameters['dataset']['zc_value'], list):\n",
        "  session_parameters['dataset']['zc_value'] = [session_parameters['dataset']['zc_value']] * (session_parameters['dataset']['zc_max_derivative_order'] + 1) # len(session_parameters['network']['ofs'])\n",
        "\n",
        "if not isinstance(session_parameters['network']['threshold'], list):\n",
        "  session_parameters['network']['threshold'] = [session_parameters['network']['threshold']] * len(session_parameters['network']['ofs'])\n",
        "\n",
        "if not isinstance(session_parameters['network']['beta_decay'], list):\n",
        "  session_parameters['network']['beta_decay'] = [session_parameters['network']['beta_decay']] * len(session_parameters['network']['ofs'])\n",
        "\n",
        "if not session_parameters['network']['threshold_train']:\n",
        "  session_parameters['network']['threshold_perlayer'] = True\n",
        "\n",
        "if not session_parameters['network']['beta_decay_train']:\n",
        "  session_parameters['network']['beta_decay_perlayer'] = True\n",
        "\n",
        "if session_parameters['network']['beta_decay_perlayer']:\n",
        "  session_parameters['network']['beta_decay_outsingle'] = True\n",
        "\n",
        "if not session_parameters['dataset']['channels_downsampling'] or session_parameters['dataset']['channels_downsampling'] == 1:\n",
        "  session_parameters['dataset']['channels_downsampling'] = None\n",
        "else:\n",
        "  session_parameters['dataset']['channels_downsampling'] = int(session_parameters['dataset']['channels_downsampling'])\n",
        "\n",
        "if not session_parameters['dataset']['force_gain']:\n",
        "  session_parameters['dataset']['force_gain'] = 1\n",
        "\n",
        "session_results['hash_name'] = hash_dictionary(session_parameters)[:8]\n",
        "session_results['colab_env'] = colab_env\n",
        "session_results['device'] =  'cpu' # str(torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')))\n",
        "\n",
        "session_folder_name = session_results['hash_name'] # if session_info['hash_rename'] else session_info['session_name']\n",
        "session_loaded = False\n",
        "\n",
        "if session_info['session_loadbyhash']:\n",
        "  session_info['overwrite'] = False\n",
        "\n",
        "if session_info['overwrite']:\n",
        "  try:\n",
        "    shutil.rmtree(os.path.join(session_info['session_output_path'], session_folder_name))\n",
        "  except:\n",
        "    pass\n",
        "  os.makedirs(os.path.join(session_info['session_output_path'], session_folder_name), exist_ok=True)\n",
        "\n",
        "  print('Session overwriting')\n",
        "\n",
        "else:\n",
        "  for session_hash in [session_info['session_loadbyhash']] if session_info['session_loadbyhash'] else [session_results['hash_name']]:\n",
        "    existing_session_folder_name = find_session_byhash(session_info['session_output_path'], session_hash)\n",
        "    if existing_session_folder_name:\n",
        "      try:\n",
        "        session_info_loaded = load_json(os.path.join(session_info['session_output_path'], existing_session_folder_name, 'session_info.json'))\n",
        "        session_parameters_loaded = load_json(os.path.join(session_info['session_output_path'], existing_session_folder_name, 'session_parameters.json'))\n",
        "        session_results_loaded = load_json(os.path.join(session_info['session_output_path'], existing_session_folder_name, 'session_results.json'))\n",
        "\n",
        "        session_info_loaded['overwrite'] = session_info['overwrite']\n",
        "        session_info_loaded['lazygit_support'] = session_info['lazygit_support']\n",
        "        session_info_loaded['dataset']['plot_exampledata'] = session_info['dataset']['plot_exampledata']\n",
        "        session_info_loaded['training']['epoch_max'] = session_info['training']['epoch_max']\n",
        "        session_info_loaded['training']['auto_unassign'] = session_info['training']['auto_unassign']\n",
        "\n",
        "        session_results_loaded['hash_name'] = session_results['hash_name']\n",
        "        session_results_loaded['colab_env'] = session_results['colab_env']\n",
        "        session_results_loaded['device'] = session_results['device']\n",
        "\n",
        "        session_info = session_info_loaded\n",
        "        session_parameters = session_parameters_loaded\n",
        "        session_results = session_results_loaded\n",
        "\n",
        "        session_folder_name = existing_session_folder_name\n",
        "\n",
        "        session_loaded = True\n",
        "        print('Existing session loaded:', os.path.join(session_info['session_output_path'], existing_session_folder_name))\n",
        "        break\n",
        "      except:\n",
        "        print('Error loading session with hash:', session_info['session_loadbyhash'])\n",
        "        raise\n",
        "\n",
        "if not session_loaded:\n",
        "  if session_info['session_loadbyhash']:\n",
        "    print('No session found with hash:', session_info['session_loadbyhash'])\n",
        "    raise\n",
        "\n",
        "  else:\n",
        "    os.makedirs(os.path.join(session_info['session_output_path'], session_folder_name), exist_ok=True)\n",
        "    print('New session created:', os.path.join(session_info['session_output_path'], session_folder_name))\n",
        "\n",
        "    save_json(session_info, os.path.join(session_info['session_output_path'], session_folder_name, 'session_info.json'))\n",
        "    save_json(session_parameters, os.path.join(session_info['session_output_path'], session_folder_name, 'session_parameters.json'))\n",
        "    save_json(session_results, os.path.join(session_info['session_output_path'], session_folder_name, 'session_results.json'), savedatetime=False)\n",
        "\n",
        "    with open(os.path.join(session_info['session_output_path'], session_folder_name, 'session_hash.txt'), 'w') as file:\n",
        "      file.write(session_results['hash_name'])\n",
        "\n",
        "try:\n",
        "  if session_results['training']['epoch_last']:\n",
        "    session_trained = True\n",
        "  else:\n",
        "    session_trained = False\n",
        "except:\n",
        "  session_trained = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB5gEtveboSu"
      },
      "source": [
        "## Session environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rwk5K7Pbsra"
      },
      "outputs": [],
      "source": [
        "if session_info['github_support'] and session_results['colab_env']:\n",
        "  # Import userdata module from google.colab\n",
        "  from google.colab import userdata\n",
        "\n",
        "  # Remove the default sample_data directory in Colab\n",
        "  %rm -rf /content/sample_data/\n",
        "\n",
        "  # Change current working directory to /content\n",
        "  %cd /content/\n",
        "\n",
        "  # Repository name on GitHub\n",
        "  repository = session_info['project_name']\n",
        "\n",
        "  # Retrieve Git credentials stored in userdata\n",
        "  github_json = userdata.get('github_json')\n",
        "  github_json = json.loads(github_json)\n",
        "\n",
        "  # Configure global Git settings with the retrieved credentials\n",
        "  !git config --global user.name {github_json['name']}\n",
        "  !git config --global user.email {github_json['email']}\n",
        "  !git config --global user.password {github_json['password']}\n",
        "\n",
        "  # Clone the GitHub repository using Git token for authentication\n",
        "  !git clone -b {session_info['repository_branch'] if session_info['repository_branch'] else 'main'} https://{github_json['token']}@github.com/{github_json['name']}/{repository}\n",
        "\n",
        "  # Change directory to the cloned repository's directory\n",
        "  %cd /content/{repository}/\n",
        "\n",
        "  if 'hd-semg-1dof-e100-f100/2.0.0/1dof_dataset/' in session_parameters['dataset']['dataset_subpath']:\n",
        "    os.makedirs(f'/content/{repository}/dataset/', exist_ok=True)\n",
        "    !git clone -b {session_info['repository_branch'] if session_info['repository_branch'] else 'main'} https://{github_json['token']}@github.com/{github_json['name']}/hd-semg-1dof-e100-f100 dataset/hd-semg-1dof-e100-f100\n",
        "\n",
        "  if 'hd-semg-1dof-e400-f400/2.0.0/1dof_dataset/' in session_parameters['dataset']['dataset_subpath']:\n",
        "    os.makedirs(f'/content/{repository}/dataset/', exist_ok=True)\n",
        "    !git clone -b {session_info['repository_branch'] if session_info['repository_branch'] else 'main'} https://{github_json['token']}@github.com/{github_json['name']}/hd-semg-1dof-e400-f400 dataset/hd-semg-1dof-e400-f400\n",
        "\n",
        "  if session_info['lazygit_support']:\n",
        "    LAZYGIT_VERSION = !echo $(curl -s \"https://api.github.com/repos/jesseduffield/lazygit/releases/latest\" | grep -Po '\"tag_name\": \"v\\K[^\"]*')\n",
        "    LAZYGIT_SOURCE = f\"https://github.com/jesseduffield/lazygit/releases/latest/download/lazygit_{LAZYGIT_VERSION[0]}_Linux_x86_64.tar.gz\"\n",
        "    !curl -Lo lazygit.tar.gz {LAZYGIT_SOURCE}\n",
        "    !tar xf lazygit.tar.gz lazygit\n",
        "    !install lazygit /usr/local/bin\n",
        "    %rm lazygit.tar.gz\n",
        "    %rm lazygit\n",
        "\n",
        "else:\n",
        "  # Change directory to main directory\n",
        "  #%cd ...\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL1wiWjydJ2x"
      },
      "source": [
        "## Dataset Processing and Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ97B1rCx_i-"
      },
      "outputs": [],
      "source": [
        "all_files = get_sorted_dbfiles(os.path.join(session_info['dataset']['dataset_path'], session_parameters['dataset']['dataset_subpath']))\n",
        "selected_files = set()\n",
        "#npsave_path = os.path.join(session_info['dataset']['dataset_path'], f'hd-semg-1dof-e{session_parameters[\"dataset\"][\"new_in_data_freq_hz\"]}-f{session_parameters[\"dataset\"][\"new_out_data_freq_hz\"]}/2.0.0/1dof_dataset/')\n",
        "\n",
        "for file in all_files:\n",
        "  extracted_info = extract_file_info(file.replace(os.path.join(session_info['dataset']['dataset_path'], session_parameters['dataset']['dataset_subpath']), ''))\n",
        "  if extracted_info != 'No match found':\n",
        "    try:\n",
        "      if (\n",
        "        (not session_parameters['dataset']['subjects'] or extracted_info['subject'] in session_parameters['dataset']['subjects']) and\n",
        "        (not session_parameters['dataset']['sessions'] or extracted_info['session'] in session_parameters['dataset']['sessions']) and\n",
        "        (not session_parameters['dataset']['objects_file'] or extracted_info['object'] in session_parameters['dataset']['objects_file']) and\n",
        "        # (not session_parameters['dataset']['objects'] or extracted_info['object'] in session_parameters['dataset']['objects']) and\n",
        "        (not session_parameters['dataset']['samples'] or extracted_info['sample'] in session_parameters['dataset']['samples'])\n",
        "      ):\n",
        "        selected_files.add(extracted_info['modified_string'])\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "# Initialize an empty dictionary to store classes found in the dataset\n",
        "classes = set()\n",
        "selected_files = natsort.natsorted(selected_files)\n",
        "\n",
        "# Initialize empty lists to collect data for training, validation, and testing sets\n",
        "x_train, x_valid, x_test = [], [], []\n",
        "y_train, y_valid, y_test = [], [], []\n",
        "x_orig_train, x_orig_valid, x_orig_test = [], [], []\n",
        "y_orig_train, y_orig_valid, y_orig_test = [], [], []\n",
        "patient_train, patient_valid, patient_test = [], [], []\n",
        "all_data_maxsample = []\n",
        "all_data = []\n",
        "s_all_data_maxsample = []\n",
        "s_all_data = []\n",
        "\n",
        "tmpcnt = 0\n",
        "\n",
        "for f in tqdm(selected_files, desc='Processing and loading Files', ncols=80):\n",
        "  extracted_info = extract_file_info(f.replace(session_parameters['dataset']['dataset_subpath'], ''))\n",
        "\n",
        "  if tmpcnt == 20:\n",
        "    break\n",
        "  tmpcnt += 1\n",
        "\n",
        "  # sEMG raw process\n",
        "  f_raw = f.replace('datatype', 'raw')\n",
        "  if session_parameters['dataset']['dataset_format'] == 'wfdb':\n",
        "    signal = np.array(wfdb.rdrecord(os.path.join(session_info['dataset']['dataset_path'], session_parameters['dataset']['dataset_subpath'], f_raw)).p_signal)\n",
        "  elif session_parameters['dataset']['dataset_format'] == 'numpy':\n",
        "    signal = np.load(os.path.join(session_info['dataset']['dataset_path'], session_parameters['dataset']['dataset_subpath'], f_raw) + '.npy')\n",
        "  else:\n",
        "    break\n",
        "\n",
        "  if session_parameters['dataset']['channel_start'] or session_parameters['dataset']['channel_end']:\n",
        "    signal = signal[:, session_parameters['dataset']['channel_start'] : session_parameters['dataset']['channel_end']]\n",
        "\n",
        "  if session_parameters['dataset']['channels_downsampling']:\n",
        "    signal = signal[:, ::session_parameters['dataset']['channels_downsampling']]\n",
        "\n",
        "  signal_maxsample = signal\n",
        "\n",
        "  if session_parameters['dataset']['new_in_data_freq_hz']:\n",
        "    signal = signal_downsample(signal, session_parameters['dataset']['in_data_freq_hz'], session_parameters['dataset']['new_in_data_freq_hz'])\n",
        "  #os.makedirs(os.path.dirname(os.path.join(npsave_path, f_raw) + '.npy'), exist_ok=True)\n",
        "  #np.save(os.path.join(npsave_path, f_raw) + '.npy', signal)\n",
        "  _, spike_signal = process_data(signal, session_parameters['dataset']['max_derivative_order'], session_parameters['dataset']['delta_value'], session_parameters['dataset']['zc_enable'], session_parameters['dataset']['zc_max_derivative_order'], session_parameters['dataset']['zc_value'])\n",
        "  \n",
        "  if extracted_info['sample'] in session_parameters['dataset']['train_samples']:\n",
        "    x_train.append(spike_signal)\n",
        "    patient_train.append(extracted_info['subject'])\n",
        "  if extracted_info['sample'] in session_parameters['dataset']['valid_samples']:\n",
        "    x_valid.append(spike_signal)\n",
        "    patient_valid.append(extracted_info['subject'])\n",
        "  if extracted_info['sample'] in session_parameters['dataset']['test_samples']:\n",
        "    x_test.append(spike_signal)\n",
        "    patient_test.append(extracted_info['subject'])\n",
        "\n",
        "  if extracted_info['sample'] in session_parameters['dataset']['train_samples']:\n",
        "    x_orig_train.append(signal)\n",
        "  if extracted_info['sample'] in session_parameters['dataset']['valid_samples']:\n",
        "    x_orig_valid.append(signal)\n",
        "  if extracted_info['sample'] in session_parameters['dataset']['test_samples']:\n",
        "    x_orig_test.append(signal)\n",
        "\n",
        "  # Force process\n",
        "  f_force = f.replace('datatype', 'force')\n",
        "  if session_parameters['dataset']['dataset_format'] == 'wfdb':\n",
        "    signal = np.array(wfdb.rdrecord(os.path.join(session_info['dataset']['dataset_path'], session_parameters['dataset']['dataset_subpath'], f_force)).p_signal)\n",
        "  elif session_parameters['dataset']['dataset_format'] == 'numpy':\n",
        "    signal = np.load(os.path.join(session_info['dataset']['dataset_path'], session_parameters['dataset']['dataset_subpath'], f_force) + '.npy')\n",
        "\n",
        "  if session_parameters['dataset']['mvc_norm']:\n",
        "    force_finger_mvc = get_mvc( '../datasets/emg/hd-semg/2.0.0', extracted_info['subject'], 1)\n",
        "    signal = mvc_norm(signal, force_finger_mvc)\n",
        "\n",
        "  if session_parameters['dataset']['objects_class']:\n",
        "    if extracted_info['object'] in session_parameters['dataset']['objects_class']:\n",
        "      classes.add(extracted_info['object'])\n",
        "    signal = signal[:, [x - 1 for x in session_parameters['dataset']['objects_class']]]\n",
        "  else:\n",
        "    classes.add(extracted_info['object'])\n",
        "\n",
        "  if session_parameters['dataset']['new_out_data_freq_hz']:\n",
        "    signal = signal_upsample(signal, session_parameters['dataset']['out_data_freq_hz'], session_parameters['dataset']['new_out_data_freq_hz'])\n",
        "    signal_orig = signal\n",
        "  if session_parameters['dataset']['out_data_filterfreq_hz']:\n",
        "    if session_parameters['dataset']['new_out_data_freq_hz']:\n",
        "      signal = lowpass_filter(signal, session_parameters['dataset']['out_data_filterfreq_hz'], session_parameters['dataset']['new_out_data_freq_hz'])\n",
        "    else:\n",
        "      signal = lowpass_filter(signal, session_parameters['dataset']['out_data_filterfreq_hz'], session_parameters['dataset']['out_data_freq_hz'])\n",
        "  #os.makedirs(os.path.dirname(os.path.join(npsave_path, f_force) + '.npy'), exist_ok=True)\n",
        "  #np.save(os.path.join(npsave_path, f_force) + '.npy', signal)\n",
        "\n",
        "  if session_parameters['dataset']['zc_enable']:\n",
        "    if session_parameters['dataset']['max_derivative_order'] or session_parameters['dataset']['zc_max_derivative_order']:\n",
        "      signal = signal[max(session_parameters['dataset']['max_derivative_order'], session_parameters['dataset']['zc_max_derivative_order']) * 2 : - max(session_parameters['dataset']['max_derivative_order'], session_parameters['dataset']['zc_max_derivative_order']) * 2, :]\n",
        "  elif session_parameters['dataset']['max_derivative_order']:\n",
        "    signal = signal[session_parameters['dataset']['max_derivative_order'] * 2 : - session_parameters['dataset']['max_derivative_order'] * 2, :]\n",
        "\n",
        "  if session_parameters['dataset']['force_gain'] != 1:\n",
        "    signal = signal * session_parameters['dataset']['force_gain']\n",
        "\n",
        "  if extracted_info['sample'] in session_parameters['dataset']['train_samples']:\n",
        "    y_train.append(signal)\n",
        "  if extracted_info['sample'] in session_parameters['dataset']['valid_samples']:\n",
        "    y_valid.append(signal)\n",
        "  if extracted_info['sample'] in session_parameters['dataset']['test_samples']:\n",
        "    y_test.append(signal)\n",
        "\n",
        "  if extracted_info['sample'] in session_parameters['dataset']['train_samples']:\n",
        "    y_orig_train.append(signal_orig)\n",
        "  if extracted_info['sample'] in session_parameters['dataset']['valid_samples']:\n",
        "    y_orig_valid.append(signal_orig)\n",
        "  if extracted_info['sample'] in session_parameters['dataset']['test_samples']:\n",
        "    y_orig_test.append(signal_orig)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "x_train, x_valid, x_test = map(np.array, [x_train, x_valid, x_test])\n",
        "y_train, y_valid, y_test = map(np.array, [y_train, y_valid, y_test])\n",
        "x_orig_train, x_orig_valid, x_orig_test = map(np.array, [x_orig_train, x_orig_valid, x_orig_test])\n",
        "y_orig_train, y_orig_valid, y_orig_test = map(np.array, [y_orig_train, y_orig_valid, y_orig_test])\n",
        "patient_train, patient_valid, patient_test = map(np.array, [patient_train, patient_valid, patient_test])\n",
        "\n",
        "# Transposing x and y datasets: swapping axis 0 with axis 2 for each dataset\n",
        "x_train, x_valid, x_test = [np.transpose(x, (0, 2, 1)) for x in [x_train, x_valid, x_test]]\n",
        "y_train, y_valid, y_test = [np.transpose(y, (0, 2, 1)) for y in [y_train, y_valid, y_test]]\n",
        "x_orig_train, x_orig_valid, x_orig_test = [np.transpose(x, (0, 2, 1)) for x in [x_orig_train, x_orig_valid, x_orig_test]]\n",
        "y_orig_train, y_orig_valid, y_orig_test = [np.transpose(y, (0, 2, 1)) for y in [y_orig_train, y_orig_valid, y_orig_test]]\n",
        "\n",
        "# Check shapes of resulting datasets\n",
        "x_train.shape, x_valid.shape, x_test.shape, y_train.shape, y_valid.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AicqcalkQpXX"
      },
      "outputs": [],
      "source": [
        "if session_info['dataset']['plot_exampledata']:\n",
        "  fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "  # Corresponding labels for each index for reference\n",
        "  labels = [\"thumb\", \"index\", \"middle\", \"ring\", \"little\"]\n",
        "\n",
        "  if not session_parameters['dataset']['objects_class']:\n",
        "    i = 0\n",
        "    while True:\n",
        "      try:\n",
        "        ax1.plot(y_test[0][i], label=labels[i])\n",
        "        i += 1\n",
        "      except:\n",
        "        break\n",
        "  else:\n",
        "    # Plot only the signals specified in the `indices_to_plot` list\n",
        "    for i, index in enumerate([x - 1 for x in session_parameters['dataset']['objects_class']]):\n",
        "        ax1.plot(y_test[0][i], label=labels[index])\n",
        "\n",
        "  ax1.legend(loc='upper left')\n",
        "  ax1.set_ylabel('forcesignal')\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHue9XnN1R4k"
      },
      "outputs": [],
      "source": [
        "# Set a random seed for reproducibility of shuffling\n",
        "random_seed = session_parameters['dataset']['random_seed']\n",
        "\n",
        "# Shuffle the training, validation, and testset dataset, both features and labels, using the specified random seed\n",
        "x_train, y_train, patient_train = shuffle(x_train, y_train, patient_train, random_state=random_seed)\n",
        "#x_valid, y_valid, patient_valid = shuffle(x_valid, y_valid, patient_valid,random_state=random_seed)\n",
        "#x_test, y_test = shuffle(x_test, y_test, random_state=random_seed)\n",
        "\n",
        "x_orig_train, y_orig_train, patient_train = shuffle(x_orig_train, y_orig_train, patient_train, random_state=random_seed)\n",
        "\n",
        "if False:\n",
        "  # Convert the shuffled numpy arrays to PyTorch tensors with dtype=torch.int8\n",
        "  x_train_tensor = torch.tensor(x_train, dtype=torch.int8)\n",
        "  y_train_tensor = torch.tensor(y_train, dtype=torch.int8)\n",
        "  x_valid_tensor = torch.tensor(x_valid, dtype=torch.int8)\n",
        "  y_valid_tensor = torch.tensor(y_valid, dtype=torch.int8)\n",
        "  x_test_tensor = torch.tensor(x_test, dtype=torch.int8)\n",
        "  y_test_tensor = torch.tensor(y_test, dtype=torch.int8)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = HDsEMG_Dataset(y_train, x_train)\n",
        "valid_dataset = HDsEMG_Dataset(y_valid, x_valid)\n",
        "test_dataset = HDsEMG_Dataset(y_test, x_test)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_orig_dataset = HDsEMG_Dataset(y_orig_train, x_orig_train)\n",
        "valid_orig_dataset = HDsEMG_Dataset(y_orig_valid, x_orig_valid)\n",
        "test_orig_dataset = HDsEMG_Dataset(y_orig_test, x_orig_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = session_parameters['training']['batch_size']\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader_1 = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = session_parameters['training']['batch_size']\n",
        "train_orig_dataloader = DataLoader(train_orig_dataset, batch_size=batch_size, shuffle=False)\n",
        "valid_orig_dataloader = DataLoader(valid_orig_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_orig_dataloader = DataLoader(test_orig_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Print out some DataLoader details to confirm\n",
        "len(train_dataloader), len(valid_dataloader), len(test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxofGkhfbG6k"
      },
      "source": [
        "## Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL7jNrR-bIxz"
      },
      "outputs": [],
      "source": [
        "if session_loaded and session_trained:\n",
        "  if session_results['training']['early_stopped'] or (session_info['training']['epoch_max'] and (session_results['training']['epoch_last'] >= (session_info['training']['epoch_max'] - 1))):\n",
        "    try:\n",
        "      model = Net(timesteps = x_train.shape[2], input = x_train.shape[1]).to(session_results['device'])\n",
        "      model.load_state_dict(torch.load(os.path.join(session_info['session_output_path'], session_folder_name, 'model_best.pt'), map_location=torch.device(session_results['device'])))\n",
        "      print(f'Model correctly loaded: {os.path.join(session_info[\"session_output_path\"], session_folder_name, \"model_best.pt\")}\\nThe training will not be performed as it has already been completed in the previous training or maximum epoch have already been reached')\n",
        "    except FileNotFoundError as e:\n",
        "      raise FileNotFoundError(f\"Model file not found in {session_info['session_output_path']}: {e}\")\n",
        "    except KeyError as e:\n",
        "      raise KeyError(f\"Missing key in session_results: {e}\")\n",
        "    except Exception as e:\n",
        "      raise Exception(f\"Error loading the model: {e}\")\n",
        "\n",
        "    session_results['training']['done'] = True\n",
        "    training_enable = False\n",
        "  else:\n",
        "    try:\n",
        "      model = Net(timesteps = x_train.shape[2], input = x_train.shape[1]).to(session_results['device'])\n",
        "      model.load_state_dict(torch.load(os.path.join(session_info['session_output_path'], session_folder_name, 'model_last.pt'), map_location=torch.device(session_results['device'])))\n",
        "      print(f'Model correctly loaded: {os.path.join(session_info[\"session_output_path\"], session_folder_name, \"model_last.pt\")}\\nThe training will be concluded')\n",
        "    except FileNotFoundError as e:\n",
        "      raise FileNotFoundError(f\"Model file not found in {session_info['session_output_path']}: {e}\")\n",
        "    except KeyError as e:\n",
        "      raise KeyError(f\"Missing key in session_results: {e}\")\n",
        "    except Exception as e:\n",
        "      raise Exception(f\"Error loading the model: {e}\")\n",
        "\n",
        "    session_results['training']['done'] = False\n",
        "    training_enable = True\n",
        "\n",
        "else:\n",
        "  session_results['network']['ofs'] = session_parameters['network']['ofs']\n",
        "  if session_results['network']['ofs'][-1] == None:\n",
        "    session_results['network']['ofs'][-1] = len(classes)\n",
        "  save_json(session_results, os.path.join(session_info['session_output_path'], session_folder_name, 'session_results.json'))\n",
        "  if session_parameters['training']['pretrained_model']:\n",
        "    model = Net(timesteps = x_train.shape[2], input = x_train.shape[1]).to(session_results['device'])\n",
        "    model.load_state_dict(torch.load(session_parameters['training']['pretrained_model'], map_location=torch.device(session_results['device'])))\n",
        "    print(f'Pretrained model correctly loaded: {session_parameters[\"training\"][\"pretrained_model\"]}\\nA new training will be performed')\n",
        "  else:\n",
        "    model = Net(timesteps = x_train.shape[2], input = x_train.shape[1]).to(session_results['device'])\n",
        "    print(f'Model correctly created\\nA new training will be performed')\n",
        "\n",
        "  training_enable = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2Np1CKh2LgV"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUxDzhAp5oPa"
      },
      "source": [
        "### Training settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9mvyD1X2N01"
      },
      "outputs": [],
      "source": [
        "error = torch.nn.MSELoss()\n",
        "\n",
        "if training_enable:\n",
        "  if session_loaded and session_trained:\n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=session_results['training']['learning_rate'][-1])\n",
        "    learning_rate = session_results['training']['learning_rate'][-1]\n",
        "\n",
        "  else:\n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=session_parameters['training']['learning_rate_first'])\n",
        "\n",
        "    session_results['training'] = {\n",
        "      'optimizer' : None,              # UNEDITABLE # Training optimizer.\n",
        "      'done': False,                   # UNEDITABLE # Flag to indicate if the training process is complete.\n",
        "      'early_stopped': False,          # UNEDITABLE # Indicates if training was stopped early due to lack of improvement.\n",
        "      'epoch_best': None,              # UNEDITABLE # Epoch number where the best performance was achieved.\n",
        "      'epoch_last': None,              # UNEDITABLE # Last epoch number before training concluded.\n",
        "      'epoch_no_imporve': None,        # UNEDITABLE # For how many epochs has the model not improved on the validation set.\n",
        "      'train_loss_min': None,          # UNEDITABLE # Best training loss achieved.\n",
        "      'valid_loss_min': None,          # UNEDITABLE # Best validation loss achieved.\n",
        "      'learning_rate_best': None,      # UNEDITABLE # Best learning rate achieved during training.\n",
        "      'train_loss_best': None,         # UNEDITABLE # Best training loss achieved in the model_best.pt.\n",
        "      'valid_loss_best': None,         # UNEDITABLE # Best validation loss achieved in the model_best.pt.\n",
        "      'learning_rate': [],             # UNEDITABLE # History of learning rates used throughout training.\n",
        "      'train_loss': [],                # UNEDITABLE # History of training losses per epoch.\n",
        "      'valid_loss': [],                # UNEDITABLE # History of validation losses per epoch.\n",
        "      'train_acc': [],                 # UNEDITABLE # History of training accuracies per epoch.\n",
        "      'valid_acc': []                  # UNEDITABLE # History of validation accuracies per epoch.\n",
        "    }\n",
        "\n",
        "    session_results['post_training'] = {\n",
        "      'test_acc': None,                # UNEDITABLE # Test accuracy achieved after training is complete.\n",
        "      'test_acc_fakequant': None       # UNEDITABLE # Test accuracy achieved with fake quantization after training.\n",
        "    }\n",
        "\n",
        "    # Configuring and saving session information\n",
        "    session_results['training']['optimizer'] = str(optimizer).replace(\"\\n\",\"\").replace(\"    \",\", \")\n",
        "    session_results['training']['epoch_no_imporve'] = 0\n",
        "    session_results['training']['train_loss_min'] = float('inf')\n",
        "    session_results['training']['valid_loss_min'] = float('inf')\n",
        "\n",
        "    save_json(session_results, os.path.join(session_info['session_output_path'], session_folder_name, 'session_results.json'))\n",
        "\n",
        "    # Setting learning rate parameter\n",
        "    learning_rate = session_parameters['training']['learning_rate_first']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSRBGVAG5l0S"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJ1gbG_r5pio"
      },
      "outputs": [],
      "source": [
        "if training_enable:\n",
        "  if session_info['training']['epoch_max']:\n",
        "    start_time = time.time()\n",
        "    if session_results['training']['epoch_last'] == None:\n",
        "      start_iter = 0\n",
        "    else:\n",
        "      start_iter = session_results['training']['epoch_last'] + 1\n",
        "    progress_bar(start_iter, session_info['training']['epoch_max'], start_time, start_iter, prefix='Epoch:', init=True)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  while True:\n",
        "    if session_info['training']['epoch_max'] and session_results['training']['epoch_last'] and (session_results['training']['epoch_last'] + 1) >= session_info['training']['epoch_max']:\n",
        "      break\n",
        "\n",
        "    # Training\n",
        "    loss_values = []\n",
        "    for i, (input, label) in enumerate(train_dataloader):\n",
        "      input = torch.swapaxes(input=input, axis0=0, axis1=1).to(session_results['device'])\n",
        "      label = torch.swapaxes(input=label, axis0=0, axis1=1).to(session_results['device'])\n",
        "\n",
        "      output = model(input)\n",
        "\n",
        "      # loss_value = error(output.float(), label.float())\n",
        "      loss_value = error(output, label)\n",
        "      optimizer.zero_grad()\n",
        "      loss_value.backward()\n",
        "      optimizer.step()\n",
        "      loss_values.append(loss_value.item())\n",
        "\n",
        "    train_loss_value = sum(loss_values) / len(loss_values)\n",
        "\n",
        "    # Validation\n",
        "    loss_values = []\n",
        "    for i, (input, label) in enumerate(valid_dataloader):\n",
        "      input = torch.swapaxes(input=input, axis0=0, axis1=1).to(session_results['device'])\n",
        "      label = torch.swapaxes(input=label, axis0=0, axis1=1).to(session_results['device'])\n",
        "\n",
        "      output = model(input)\n",
        "\n",
        "      # loss_value = error(output.float(), label.float())\n",
        "      loss_value = error(output, label)\n",
        "      loss_values.append(loss_value.item())\n",
        "\n",
        "    valid_loss_value = sum(loss_values) / len(loss_values)\n",
        "\n",
        "    if session_results['training']['epoch_last'] == None:\n",
        "      session_results['training']['epoch_last'] = 0\n",
        "    else:\n",
        "      session_results['training']['epoch_last'] += 1\n",
        "\n",
        "    # Session checkpoint\n",
        "    torch.save(model.state_dict(), os.path.join(session_info['session_output_path'], session_folder_name, 'model_last.pt'))\n",
        "\n",
        "    train_best_loss, valid_best_loss = False, False\n",
        "    session_results['training']['learning_rate'].append(learning_rate)\n",
        "    session_results['training']['train_loss'].append(train_loss_value)\n",
        "    session_results['training']['valid_loss'].append(valid_loss_value)\n",
        "    if session_results['training']['train_loss_min'] > train_loss_value:\n",
        "      session_results['training']['train_loss_min'] = train_loss_value\n",
        "      train_best_loss = True\n",
        "    if session_results['training']['valid_loss_min'] > valid_loss_value:\n",
        "      session_results['training']['valid_loss_min'] = valid_loss_value\n",
        "      valid_best_loss = True\n",
        "\n",
        "    save_json(session_results, os.path.join(session_info['session_output_path'], session_folder_name, 'session_results.json'))\n",
        "\n",
        "    if session_info['training']['epoch_max']:\n",
        "      progress_bar(session_results['training']['epoch_last'] + 1, session_info['training']['epoch_max'], start_time, start_iter, prefix='Epoch:', suffix=f'TL: {safe_format(train_loss_value)} ({safe_format(session_results[\"training\"][\"train_loss_min\"])}), VL: {safe_format(valid_loss_value)} ({safe_format(session_results[\"training\"][\"valid_loss_min\"])}).')\n",
        "    else:\n",
        "      print(\n",
        "        f'\\rEpoch {session_results[\"training\"][\"epoch_last\"]}.   ' +                                                      # Epoch\n",
        "        f'Train loss: {safe_format(train_loss_value)} ({safe_format(session_results[\"training\"][\"train_loss_min\"])}), ' + # Training loss\n",
        "        f'Valid loss: {safe_format(valid_loss_value)} ({safe_format(session_results[\"training\"][\"valid_loss_min\"])}). ',  # Validation loss\n",
        "        end=''\n",
        "      )\n",
        "\n",
        "    # Check best model\n",
        "    if (session_parameters['training']['best_loss'] and train_best_loss) or (not session_parameters['training']['best_loss'] and valid_best_loss):\n",
        "      torch.save(model.state_dict(), os.path.join(session_info['session_output_path'], session_folder_name, 'model_best.pt'))\n",
        "\n",
        "      session_results['training']['learning_rate_best'] = learning_rate\n",
        "      session_results['training']['train_loss_best'] = train_loss_value\n",
        "      session_results['training']['valid_loss_best'] = valid_loss_value\n",
        "      session_results['training']['epoch_best'] = session_results['training']['epoch_last']\n",
        "\n",
        "      save_json(session_results, os.path.join(session_info['session_output_path'], session_folder_name, 'session_results.json'))\n",
        "\n",
        "      session_results['training']['epoch_no_imporve'] = 0\n",
        "\n",
        "    else:\n",
        "      session_results['training']['epoch_no_imporve'] += 1\n",
        "\n",
        "      if session_results['training']['epoch_no_imporve'] >= session_parameters['training']['patience']:\n",
        "        if learning_rate >= round((session_parameters['training']['learning_rate_decay'] ** session_parameters['training']['learning_rate_decay_steps']) * session_parameters['training']['learning_rate_first'], 16):\n",
        "          learning_rate = round(learning_rate * session_parameters['training']['learning_rate_decay'], 16)\n",
        "\n",
        "          if session_info['training']['bestmodel_lrchange']:\n",
        "            try:\n",
        "              model.load_state_dict(torch.load(os.path.join(session_info['session_output_path'], session_folder_name, 'model_best.pt'), map_location=torch.device(session_results['device'])))\n",
        "            except:\n",
        "              pass\n",
        "\n",
        "          save_json(session_results, os.path.join(session_info['session_output_path'], session_folder_name, 'session_results.json'))\n",
        "          for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = learning_rate\n",
        "          session_results['training']['epoch_no_imporve'] = 0\n",
        "          if session_info['training']['epoch_max']:\n",
        "            progress_bar(session_results['training']['epoch_last'] + 1, session_info['training']['epoch_max'], start_time, start_iter, prefix='Epoch:', suffix=f'TL: {safe_format(train_loss_value)} ({safe_format(session_results[\"training\"][\"train_loss_min\"])}), VL: {safe_format(valid_loss_value)} ({safe_format(session_results[\"training\"][\"valid_loss_min\"])}). Reducing learning rate: {learning_rate}\\n')\n",
        "          else:\n",
        "            print(f'Reducing learning rate:', learning_rate)\n",
        "        else:\n",
        "          session_results['training']['early_stopped'] = True\n",
        "          save_json(session_results, os.path.join(session_info['session_output_path'], session_folder_name, 'session_results.json'))\n",
        "          if session_info['training']['epoch_max']:\n",
        "            progress_bar(session_results['training']['epoch_last'] + 1, session_info['training']['epoch_max'], start_time, start_iter, prefix='Epoch:', suffix=f'TL: {safe_format(train_loss_value)} ({safe_format(session_results[\"training\"][\"train_loss_min\"])}), VL: {safe_format(valid_loss_value)} ({safe_format(session_results[\"training\"][\"valid_loss_min\"])}). Early stopping triggered after {session_results[\"training\"][\"epoch_last\"]+1} epochs!')\n",
        "          else:\n",
        "            print(f'Early stopping triggered after {session_results[\"training\"][\"epoch_last\"]+1} epochs!')\n",
        "          break\n",
        "      else:\n",
        "        if session_info['training']['epoch_max']:\n",
        "          progress_bar(session_results['training']['epoch_last'] + 1, session_info['training']['epoch_max'], start_time, start_iter, prefix='Epoch:', suffix=f'TL: {safe_format(train_loss_value)} ({safe_format(session_results[\"training\"][\"train_loss_min\"])}), VL: {safe_format(valid_loss_value)} ({safe_format(session_results[\"training\"][\"valid_loss_min\"])}). Patience: {session_results[\"training\"][\"epoch_no_imporve\"]}.')\n",
        "        else:\n",
        "          print(f'Patience: {session_results[\"training\"][\"epoch_no_imporve\"]}.', end='')\n",
        "        pass\n",
        "\n",
        "  session_results['training']['done'] = True\n",
        "  save_json(session_results, os.path.join(session_info['session_output_path'], session_folder_name, 'session_results.json'))\n",
        "  model.load_state_dict(torch.load(os.path.join(session_info['session_output_path'], session_folder_name, 'model_best.pt'), map_location=torch.device(session_results['device'])))\n",
        "\n",
        "  if colab_env and session_info['training']['auto_unassign']:\n",
        "    time.sleep(30)\n",
        "    runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDM0dHZ2JUQ4"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCpVG4XKpBN_"
      },
      "source": [
        "### Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13ZpDNdk4hOj"
      },
      "outputs": [],
      "source": [
        "# Create a figure and axis\n",
        "plt.figure(figsize=(16, 4))\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(session_results['training']['train_loss'], label='Training Loss')\n",
        "plt.plot(session_results['training']['valid_loss'], label='Validation Loss')\n",
        "\n",
        "# Adding title and labels\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "# Adding legend\n",
        "plt.legend()\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Jn2H3DdoSrQP",
        "ULqSulCP3IiN",
        "SegiEXtmEyWM",
        "iUxDzhAp5oPa"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
